import sys
import json
import streamlit as st
import streamlit.components.v1 as components 
import pandas as pd
import numpy as np
import datetime
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import MinMaxScaler
import shap
import io
import time
import subprocess
import threading
import queue
import signal
import os
import gc
import importlib
from glob import glob
import pickle
import joblib
import webbrowser
import hashlib
import shlex
import time
import shutil
import re
from io import StringIO
import threading
import urllib.parse
import psutil
import difflib
from pathlib import Path

from reportlab.platypus import * 
from reportlab.lib.styles import * 
from reportlab.lib.pagesizes import * 
from reportlab.lib.units import * 
from reportlab.pdfbase import pdfmetrics
from reportlab.pdfbase.cidfonts import UnicodeCIDFont
pdfmetrics.registerFont(UnicodeCIDFont("HYSMyeongJo-Medium"))


########## Function í•¨ìˆ˜ ì‘ì„± ############
def clean_memory():
        gc.collect()
        torch.cuda.empty_cache()
        torch.cuda.reset_max_memory_allocated()
        torch.cuda.reset_peak_memory_stats()
        print(f"After empty_cache: {torch.cuda.memory_reserved() / 1024**2:.2f} MB")
        if torch.cuda.is_available():
            device = torch.device('cuda')
            print(f"GPU ì´ë¦„: {torch.cuda.get_device_name(device)}")
            allocated = torch.cuda.memory_allocated(device) / 1024**2  # MB ë‹¨ìœ„
            reserved = torch.cuda.memory_reserved(device) / 1024**2
            total = torch.cuda.get_device_properties(device).total_memory / 1024**2
            print(f"í˜„ì¬ ì‚¬ìš© ì¤‘ (allocated): {allocated:.2f} MB")
            print(f"ì˜ˆì•½ë¨ (reserved): {reserved:.2f} MB")
            print(f"ì´ ë©”ëª¨ë¦¬: {total:.2f} MB")
        else:
            print("GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        for i in range(10):
            torch.cuda.empty_cache()
            gc.collect()
        print(gc.collect())


def convert_to_df(data, column_name):
    if isinstance(data, pd.Series):
        df = data.reset_index()
        df.columns = ["original_index", column_name]
        return df
    elif isinstance(data, str):
        cleaned = re.sub(r"^original_index\s*", "", data.strip())
        cleaned = re.sub(r"Name:.*$", "", cleaned, flags=re.MULTILINE).strip()
        df = pd.read_csv(StringIO(cleaned),sep=r"\s+",header=None,names=["original_index", column_name])
        df["original_index"] = df["original_index"].astype(int)
        return df
    else:
        raise ValueError(f"Unsupported data type for {column_name}: {type(data)}")


def load_file(file):
    file_extension = file.name.split('.')[-1].lower()
    if file_extension == "csv":
        df = pd.read_csv(file)
    elif file_extension in ["xlsx", "xls"]:
        df = pd.read_excel(file)
        # Excel -> CSV ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ì œê³µ
        csv_buffer = io.StringIO()
        df.to_csv(csv_buffer, index=False)
        st.download_button(
            label=f"â¬‡ï¸ {file.name} CSVë¡œ ë‹¤ìš´ë¡œë“œ",
            data=csv_buffer.getvalue(),
            file_name=file.name.rsplit('.', 1)[0] + ".csv",
            mime="text/csv"
        )
    else:
        st.error("ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤.")
        df = None
    return df


def preprocess_and_save(uploaded_files, module, mode, task_name):
    st.session_state.preprocess_messages = [
        msg for msg in st.session_state.preprocess_messages if not msg.startswith(f"âœ… {mode}")]
    feature_type = st.session_state.common_feature_type 
    drive_pattern = st.session_state.drive_pattern
    gvw = st.session_state.common_gvw
    min_length = st.session_state.common_min_length
    engine_threshold = st.session_state.engine_threshold
    engine_threshold_high = st.session_state.engine_threshold_high
    fuel_threshold = st.session_state.fuel_threshold

    base_dir = os.path.join(BASE_DIR,"data_preprocessed") # os.abspath ì¶”ê°€ 
    if mode == "train":
        save_dir = os.path.join(base_dir, f"normal_gvw{gvw}", feature_type, drive_pattern)
    elif mode == "eval":
        save_dir = os.path.join(base_dir, f"Eval_{task_name}", feature_type, drive_pattern)
    elif mode =='fine_tune':
        save_dir = os.path.join(base_dir, f"normal_gvw{gvw}_fine_tune", feature_type, drive_pattern)
    elif mode == "test":
        save_dir = os.path.join(base_dir, task_name, feature_type, drive_pattern)
    elif mode == "normal_calibration":
        save_dir = os.path.join(base_dir, "normal_engine_B", feature_type, drive_pattern)
    else:
        raise ValueError(f"Unknown mode: {mode}")

    os.makedirs(save_dir, exist_ok=True)
    save_path = os.path.join(save_dir, "data.csv")
    uploaded_files = sorted(uploaded_files, key=lambda x: x.name.lower())
    df_list = []
    total_files = len(uploaded_files)
    progress = st.progress(0)
    with st.spinner(f"{mode} ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘..."):
        status_text = st.empty()
        for i, f in enumerate(uploaded_files):
            try:
                f.seek(0)
            except Exception:
                pass
            try:
                if hasattr(module, "load_file"):
                    df = module.load_file(f)
                else:
                    df = pd.read_csv(f)
            except Exception as e:
                st.error(f"âš  íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {f.name}\n{e}")
                continue

            if df is None or df.empty:
                st.warning(f"âš  íŒŒì¼ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤: {f.name}")
                continue
            # NaN ì²˜ë¦¬
            df = module.NaN_to_value(df)
            df["file_number"] = i
            df_list.append(df)
            percent = int((i + 1) / total_files * 40)
            progress.progress(percent)
            status_text.text(f"íŒŒì¼ ì²˜ë¦¬ ì¤‘: {i + 1}/{total_files} ({f.name})")
        if len(df_list) == 0:
            st.error("âš  ì „ì²˜ë¦¬ ê°€ëŠ¥í•œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. (EmptyDataError ì˜ˆë°©ë¨)")
            return None, None
        all_df = pd.concat(df_list, ignore_index=True)
        status_text.text("ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì ìš© ì¤‘...")
        df_processed = module.preprocess_pipeline(all_df,drive_pattern,min_length,engine_threshold,engine_threshold_high,fuel_threshold)
        progress.progress(70)
        if df_processed is None or df_processed.empty:
            st.warning("âš  ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. í˜„ì¬ ë°ì´í„°ë¥¼ ë‹¤ì‹œ í™•ì¸í•˜ì„¸ìš”.")
            st.info("ì „ì²˜ë¦¬ ë° ì£¼í–‰íŒ¨í„´ ì„¤ì • ê²°ê³¼, ì „ì²˜ë¦¬ ì´í›„ ë°ì´í„° ê°œìˆ˜ê°€ 0ê°œë¡œ í™•ì¸ëìŠµë‹ˆë‹¤.")
            return None, None
        if feature_type == "main_feature":
            MAIN_FEATURE = [
                'Air_temp_Value','Air_pres_Value','Exhaust_temp_Value','Exhaust_pres_Value',
                'Turbo_speed','J1939CM_VNT_Position','EGR_Position_demand',
                'J1939CM_Egr_position','EGR_Gas_temp_Value','TSE_Turbine_in_temp']
            keep_cols = MAIN_FEATURE + ['file_number','original_index','segment_original_start','segment_original_end']
            df_processed = df_processed[keep_cols]

        try:
            status_text.text("CSV ì €ì¥ ì¤‘...")
            df_processed.to_csv(save_path, index=False)
            progress.progress(90)
            exclude_cols = ['file_number','attack','original_index','segment_original_start','segment_original_end']
            column_list_path = os.path.join(save_dir, 'column_list.txt')
            with open(column_list_path, "w", encoding="utf-8") as f:
                for col in df_processed.columns:
                    if col not in exclude_cols:
                        f.write(f"{col}\n")
            status_text.text("ì—°ì† ì¸ë±ìŠ¤ ê·¸ë£¹ ë¶„ì„ ì¤‘...")
            module.analyze_consecutive_index_groups_by_condition(
                df_processed, task_name, drive_pattern, save_dir)
            progress.progress(100)
        except PermissionError:
            st.error(f"âš  íŒŒì¼ ì €ì¥ ê¶Œí•œ ì˜¤ë¥˜: {save_path}")
            return None, None
        status_text.text("âœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ")

        # # # --- ì„¸ì…˜ì— Threshold ë° min_length ê°’ ì €ì¥ ---
        st.session_state["engine_threshold"] = engine_threshold
        st.session_state["engine_threshold_high"] = engine_threshold_high
        st.session_state["fuel_threshold"] = fuel_threshold
        st.session_state["feature_type"] = feature_type
        st.session_state["gvw"] = gvw
        st.session_state["drive_pattern"] = drive_pattern

        if mode == "train":
            st.session_state["df_train_preview"] = df_processed
            st.session_state["task_name_train"] = task_name
        elif mode =="eval":
            st.session_state["df_eval_preview"] = df_processed
            st.session_state["task_name_eval"] = task_name
        elif mode == "test":
            st.session_state["df_test_preview"] = df_processed
            st.session_state["task_name_test"] = task_name
            save_path_txt = "test_session_info.txt"
            with open(save_path_txt, "w", encoding="utf-8") as f:
                f.write(f"{feature_type}\n")
                f.write(f"{st.session_state.common_level}\n")
                f.write(f"{gvw}\n")
                f.write(f"{task_name}\n")
        elif mode == "normal_calibration":
            st.session_state["df_calibration_preview"] = df_processed
            st.session_state["task_name_calibration"] = task_name
    return df_processed, save_dir


def data_upload_ui(col, label, key_prefix, mode, task_name, module):
    if mode =='test':
        col.markdown(f"###### ğŸ“‚ {label} ë°ì´í„° ì—…ë¡œë“œ (ì—”ì§„ì´ìƒíƒì§€ ìœ í˜•: {task_name})")
    elif mode=='eval':
        col.markdown(f"###### ğŸ“‚ Validation ë°ì´í„° ì—…ë¡œë“œ (ì—”ì§„ì´ìƒíƒì§€ ìœ í˜•: {task_name})")
    elif mode=='eval':
        col.markdown(f"###### ğŸ“‚ Validation ë°ì´í„° ì—…ë¡œë“œ (ì—”ì§„ì´ìƒíƒì§€ ìœ í˜•: {task_name})")
    elif mode =='normal_calibration':
        col.markdown(f"###### ğŸ“‚ Normal Calibration ë°ì´í„° ì—…ë¡œë“œ")
    else:
        col.markdown(f"###### ğŸ“‚ {label} ë°ì´í„° ì—…ë¡œë“œ")

    # --- uploader í‚¤ ê´€ë¦¬ ---
    if f"{key_prefix}_files_key" not in st.session_state:
        st.session_state[f"{key_prefix}_files_key"] = 0
    uploader_key = f"{key_prefix}_files_{st.session_state[f'{key_prefix}_files_key']}"

    # --- íŒŒì¼ ì—…ë¡œë” ---
    uploaded_files = col.file_uploader(
        f"{label} íŒŒì¼ ì—…ë¡œë“œ",
        type=["csv"],
        accept_multiple_files=True,
        key=uploader_key)
    if uploaded_files:
        st.session_state[f"{key_prefix}_uploaded"] = uploaded_files

    files_to_process = st.session_state.get(f"{key_prefix}_uploaded")
    if files_to_process:
        if col.button(f"ğŸ—‘ ë°ì´í„° ì´ˆê¸°í™”", key=f"{key_prefix}_reset"):
            keys_to_delete = [k for k in st.session_state.keys() if k.startswith(key_prefix)]
            for k in keys_to_delete:
                del st.session_state[k]
            if mode =='train':
                extra_keys = ["df_train_preview","train_save_dir","train_csv_path","column_list","saved_params","preprocess_done","params_synced"]
            elif mode == 'eval':
                extra_keys = ["df_eval_preview","eval_save_dir","eval_csv_path","eval_column_list","saved_params","preprocess_done","params_synced"]
            elif mode =='fine_tune':
                extra_keys = ["df_fine_tune_preview","fine_tune_save_dir","fine_tune_csv_path","fine_tune_column_list","saved_params","preprocess_done","params_synced"]
            else:
                extra_keys = []
            for k in extra_keys:
                if k in st.session_state:
                    del st.session_state[k]
            st.session_state[f"{key_prefix}_files_key"] = st.session_state.get(f"{key_prefix}_files_key", 0) + 1
            st.experimental_rerun()
            return None, None

    if files_to_process:
        if module is None:
            col.warning("âš  ì „ì²˜ë¦¬ ëª¨ë“ˆì„ ë¨¼ì € ì—…ë¡œë“œí•˜ì„¸ìš”.")
            return None, None
        if col.button(f"ğŸš€ ì „ì²˜ë¦¬ ì‹¤í–‰", key=f"{key_prefix}_process"):
            df, save_dir = preprocess_and_save(files_to_process, module, mode, task_name)
            if save_dir:
                st.session_state[f"{key_prefix}_save_dir"] = save_dir
            st.session_state[f"df_{key_prefix}_preview"] = df
            return df, save_dir
    return None, None


def run_subprocess_and_stream(cmd, env=None):
    st.info(f"ì‹¤í–‰ ëª…ë ¹ì–´: {cmd}")
    log_box = st.empty()
    text_container = ""
    if isinstance(cmd, str):
        cmd_parsed = shlex.split(cmd)
    else:
        cmd_parsed = cmd
    p = subprocess.Popen(cmd_parsed,stdout=subprocess.PIPE, stderr=subprocess.STDOUT,env=env, universal_newlines=True)
    try:
        for line in iter(p.stdout.readline, ""):
            if line == "" and p.poll() is not None:
                break
            text_container += line
            log_box.text_area("ì‹¤ì‹œê°„ ë¡œê·¸", value=text_container[-50000:], height=400)
        p.wait()
    except Exception as e:
        p.kill()
        text_container += f"\n[ERROR] {e}\n"
    return p.returncode, text_container


def find_latest_train_checkpoint(result_dir, task_name):
    pattern = os.path.join(result_dir, task_name, "train_window_*", "checkpoint.ckpt")
    matches = glob(pattern)
    if not matches:
        return None
    matches.sort(key=os.path.getmtime, reverse=True)
    return matches[0]

def find_latest_test_result(result_dir, task_name):
    pattern = os.path.join(result_dir, task_name, "test_window_*", "results_test.pkl")
    matches = glob(pattern)
    if not matches:
        return None
    matches.sort(key=os.path.getmtime, reverse=True)
    return matches[0]


def activate_model_parameter_sidebar():
    with st.sidebar.expander("ğŸ“Œ TranAD íŒŒë¼ë¯¸í„° ì„¤ì •", expanded=True):
        # ---------------- Epoch ----------------
        if "epoch" not in st.session_state:
            st.session_state.epoch = 1
        def sync_epoch_slider():
            st.session_state.epoch = st.session_state.epoch_slider
            st.session_state.epoch_input = st.session_state.epoch
        def sync_epoch_input():
            st.session_state.epoch = st.session_state.epoch_input
            st.session_state.epoch_slider = st.session_state.epoch

        col1, col2 = st.columns([2, 1])
        with col1:
            st.slider("Epoch ìˆ˜", 1, 100, value=st.session_state.epoch,step=1, key="epoch_slider", on_change=sync_epoch_slider)
        with col2:
            st.number_input(" ", 1, 100, value=st.session_state.epoch,step=1, key="epoch_input", on_change=sync_epoch_input)

        # ---------------- Window Size ----------------
        if "window_size" not in st.session_state:
            st.session_state.window_size = 60
        def sync_window_slider():
            st.session_state.window_size = st.session_state.window_slider
            st.session_state.window_input = st.session_state.window_size
        def sync_window_input():
            st.session_state.window_size = st.session_state.window_input
            st.session_state.window_slider = st.session_state.window_size
        col1, col2 = st.columns([2, 1])
        with col1:
            st.slider("Window Size", 10, 200, value=st.session_state.window_size,step=1, key="window_slider", on_change=sync_window_slider)
        with col2:
            st.number_input(" ", 10, 200, value=st.session_state.window_size,step=1, key="window_input", on_change=sync_window_input)

        # ---------------- Batch Size ----------------
        if "batch_size" not in st.session_state:
            st.session_state.batch_size = 64
        def sync_batch_slider():
            st.session_state.batch_size = st.session_state.batch_slider
            st.session_state.batch_input = st.session_state.batch_size
        def sync_batch_input():
            st.session_state.batch_size = st.session_state.batch_input
            st.session_state.batch_slider = st.session_state.batch_size
        col1, col2 = st.columns([2, 1])
        with col1:
            st.slider("Batch Size", 16, 1028, value=st.session_state.batch_size,step=1, key="batch_slider", on_change=sync_batch_slider)
        with col2:
            st.number_input(" ", 16, 1028, value=st.session_state.batch_size,step=1, key="batch_input", on_change=sync_batch_input)

        # ---------------- Learning Rate ----------------
        if "learning_rate" not in st.session_state:
            st.session_state.learning_rate = 1e-3
        def sync_lr_slider():
            st.session_state.learning_rate = st.session_state.lr_slider
            st.session_state.lr_input = st.session_state.learning_rate
        def sync_lr_input():
            st.session_state.learning_rate = st.session_state.lr_input
            st.session_state.lr_slider = st.session_state.learning_rate
        col1, col2 = st.columns([2, 1])
        with col1:
            st.slider("Learning Rate", 1e-6, 1e-1, value=st.session_state.learning_rate,step=1e-5, format="%.6f", key="lr_slider", on_change=sync_lr_slider)
        with col2:
            st.number_input(" ", 1e-6, 1e-1, value=float(st.session_state.learning_rate),step=1e-5, format="%.6f", key="lr_input", on_change=sync_lr_input)

        # ---------------- Session ì €ì¥ ----------------
        epoch = st.session_state.epoch 
        window_size= st.session_state.window_size
        batch_size= st.session_state.batch_size
        learning_rate = st.session_state.learning_rate
        st.session_state["epoch"] = epoch
        st.session_state["window_size"] = window_size
        st.session_state["batch_size"] = batch_size
        st.session_state["learning_rate"] = learning_rate
    return epoch, window_size, batch_size, learning_rate



def sync_model_params_from_ui():
    st.session_state.epoch = st.session_state.get("epoch_input", st.session_state.epoch)
    st.session_state.window_size = st.session_state.get("window_input", st.session_state.window_size)
    st.session_state.batch_size = st.session_state.get("batch_input", st.session_state.batch_size)
    st.session_state.learning_rate = st.session_state.get("lr_input", st.session_state.learning_rate)


def train_full_run():
    col1, col2 = st.columns([15, 1])
    with col1:
        df_train_normal, train_save_dir_normal = data_upload_ui(st,"Train","train_normal",mode="train",task_name="Normal",module=module)
    with col2:
        st.button("?", help="í˜„ì¬ ì •ìƒ ì—”ì§„ ë°ì´í„°ì— ëŒ€í•œ ì „ì²˜ë¦¬ë¥¼ Sidebar ë©”ë‰´ì˜ âš™ï¸ê³µí†µ ì„¤ì •/ğŸ”§ì£¼í–‰íŒ¨í„´ ì„¸ë¶€ì‚¬í•­ ì„¤ì •ê°’ì„ í† ëŒ€ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
    if df_train_normal is not None:
        st.session_state["df_train_preview"] = df_train_normal
    if train_save_dir_normal is not None:
        st.session_state["train_save_dir"] = train_save_dir_normal
    try:
        task_name_train = st.session_state.get("task_name_train")
        task_name_test = st.session_state.get("test_task_fault")
        feature_type = st.session_state.get("feature_type")
        gvw = st.session_state.get("gvw")
        drive_pattern = st.session_state.get("drive_pattern")
        train_path_dir = st.session_state.get("train_save_dir")
        if train_path_dir:
            train_csv = os.path.join(train_path_dir, "data.csv")
            column_list_path = os.path.join(train_path_dir, "column_list.txt")
            st.session_state["train_csv_path"] = train_csv
            st.session_state["column_list"] = column_list_path
        else:
            train_csv = st.session_state.get("train_csv_path")
            column_list_path = st.session_state.get("column_list")

        df_train_preview = st.session_state.get("df_train_preview")
        if df_train_preview is not None and train_csv:
            with open(column_list_path, "w", encoding="utf-8") as f:
                for col in df_train_preview.columns:
                    if col not in ['file_number','attack','original_index','segment_original_start','segment_original_end']:
                        f.write(f"{col}\n")
            st.session_state["df_train_preview"] = df_train_preview
            st.session_state["train_csv_path"] = train_csv
            st.session_state["column_list"] = column_list_path
            st.success(f"âœ… Train ë°ì´í„° ì €ì¥ ì™„ë£Œ: {train_csv}")
            st.session_state.preprocess_done = True
            try:
                data_params = {
                    "Data Shape": df_train_preview.shape,
                    "Train Task": task_name_train,
                    "Test Task": task_name_test,
                    "Feature Type": feature_type,
                    "GVW": gvw,
                    "Drive Pattern": drive_pattern,
                    "Min Length": common_min_length,
                    "Engine Threshold": engine_threshold,
                    "Engine Threshold High": engine_threshold_high,
                    "Fuel Threshold": fuel_threshold,
                }
            except Exception as e:
                st.error(f"âš  data_params ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
                data_params = {}

            st.session_state.data_params = data_params
            norm_saved = {k: "" if v is None else str(v) for k, v in data_params.items()}
            st.session_state.saved_params = norm_saved
            df_params = pd.DataFrame([(k, v) for k, v in norm_saved.items()], columns=["Parameter", "Value"])
            st.session_state.df_params = df_params
            st.session_state.params_synced = True
        else:
            if not st.session_state.preprocess_done:
                st.warning("âš  Train ë°ì´í„°ê°€ ì¡´ì¬í•˜ì§€ ì•Šê±°ë‚˜ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        df_params = st.session_state.get("df_params")
        if st.session_state.params_synced:
            toggle_key = "df_params_toggle"
            default_toggle = st.session_state.get(toggle_key, True)
            col1, col2 = st.columns([15, 1])
            with col1:
                show_params = st.checkbox("ğŸ“Š Project Management / ì „ì²˜ë¦¬ ê³µí†µ ì„¤ì • ë° ì£¼í–‰íŒ¨í„´ ì„¸ë¶€ì‚¬í•­", value=default_toggle, key=toggle_key)
            with col2:
                st.button("?", help="âš™ï¸ ê³µí†µ ì„¤ì • / ğŸ”§ ì£¼í–‰íŒ¨í„´ ì„¸ë¶€ì‚¬í•­ ì „ì²˜ë¦¬ ì„¤ì •ì— ëŒ€í•œ ì €ì¥ëœ í•™ìŠµë°ì´í„°ê°€ ì¡´ì¬í•˜ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.")
            if show_params:
                if df_params is not None and not df_params.empty:
                    rows, cols = df_train_preview.shape
                    st.markdown(
                        f"""
                        <div style="
                            padding:12px;
                            border-radius:10px;
                            font-size:13px;
                            color:inherit;">
                        
                        <b>ğŸ“ Project Management</b><br>
                        â€¢ ì—”ì§„ ì´ìƒíƒì§€ ìœ í˜•&nbsp;&nbsp;: {task_name_test}</code><br><br>

                        <hr style='border:2px solid #999;'>

                        <b>âš™ï¸ ê³µí†µ ì„¤ì •</b><br>
                        â€¢ Feature Type : {feature_type}</code><br>
                        â€¢ GVW : {gvw}</code><br>
                        â€¢ Drive Pattern : {drive_pattern}</code><br><br>
                        
                        <b>ğŸ”§ ì£¼í–‰íŒ¨í„´ ì„¸ë¶€ì‚¬í•­ ì„¤ì •</b><br>
                        â€¢ Engine Threshold:
                        {engine_threshold}</code> /
                        {engine_threshold_high}</code><br>
                        â€¢ Fuel Threshold: {fuel_threshold}</code><br>
                        â€¢ Min Length : {common_min_length}</code><br><br>

                        <b>ğŸ“Š í•™ìŠµ ë°ì´í„° í¬ê¸°</b><br>
                        â€¢ Data Shape&nbsp;&nbsp;: ({rows}, {cols})</code><br><br>
                        </div>
                        """,
                        unsafe_allow_html=True)
                    csv_data = df_train_preview.to_csv(index=False, encoding="utf-8-sig")
                    st.download_button(label="â¬‡ï¸ í•™ìŠµ ë°ì´í„° CSV ë‹¤ìš´ë¡œë“œ",data=csv_data,file_name="train_data.csv",mime="text/csv")
                    st.success("âœ… Task ë° ì£¼í–‰íŒ¨í„´ ë“± ì •ë³´ ë³€ê²½ ì‹œ ì „ì²˜ë¦¬ ì¬ì‹¤í–‰ í•„ìˆ˜")


        if df_train_preview is not None and not df_train_preview.empty:
            toggle_df_key = "df_train_preview_toggle"
            default_toggle_df = st.session_state.get(toggle_df_key, False)
            show_summary = st.checkbox("ğŸ“Š í•™ìŠµ ë°ì´í„° ìš”ì•½ í†µê³„ëŸ‰", value=default_toggle_df, key=toggle_df_key)
            if show_summary:
                df_summary = df_train_preview.drop(['file_number','original_index','segment_original_start','segment_original_end'],axis=1).describe()
                st.dataframe(df_summary, width=1200)  
                st.success("âœ… ìš”ì•½ í†µê³„ëŸ‰ í™•ì¸ ì™„ë£Œ")

            
        if df_train_preview is not None and not df_train_preview.empty:
            toggle_plot_key = "df_train_plot_toggle"
            default_toggle_plot = st.session_state.get(toggle_plot_key, False)
            show_plot = st.checkbox("ğŸ“ˆ í•™ìŠµ ë°ì´í„° ì‹œê°í™”", value=default_toggle_plot, key=toggle_plot_key)
            if show_plot:
                numeric_cols = df_train_preview.drop(['file_number','original_index','segment_original_start','segment_original_end'],axis=1).select_dtypes(include="number").columns.tolist()
                max_len = len(df_train_preview)
                if "range_idx" not in st.session_state:
                    st.session_state.range_idx = (0, max_len)
                    
                start_idx, end_idx = st.slider("Index Range",min_value=0,max_value=max_len,step=1,key="range_idx") #value=st.session_state.range_idx
                if start_idx > end_idx:
                    st.session_state.range_idx = (end_idx, end_idx)
                    start_idx, end_idx = end_idx, end_idx

                if "selected_multi_cols" not in st.session_state:
                    st.session_state.selected_multi_cols = []
                multi_options = ["ğŸ”„ ì „ì²´ ë³€ìˆ˜ ì„ íƒ"] + numeric_cols
                selected_cols = st.multiselect("ğŸ“ˆ ì‹œê°í™”í•  ë³€ìˆ˜ë“¤ ì„ íƒ",multi_options,key="selected_multi_cols") #default=st.session_state.selected_multi_cols

                if "ğŸ”„ ì „ì²´ ë³€ìˆ˜ ì„ íƒ" in selected_cols:
                    selected_cols = numeric_cols
                if selected_cols:
                    with st.expander("ğŸ“Š Plot ê²°ê³¼ ë³´ê¸°", expanded=True):
                        for col in selected_cols:
                            sns.set(font_scale=1.0)
                            fig, ax = plt.subplots(figsize=(12, 5))
                            ax.plot(df_train_preview[col].iloc[start_idx:end_idx],color='blue')
                            ax.set_title(f"{col} Plot (Index {start_idx} ~ {end_idx})")
                            ax.set_xlabel("Index")
                            ax.set_ylabel(col)
                            st.pyplot(fig)
                            buf = io.BytesIO()
                            fig.savefig(buf, format="png")
                            buf.seek(0)
                            st.download_button(label=f"ğŸ“¥ {col} Plot Download",data=buf,file_name=f"{col}_Train_plot.png",mime="image/png")

    except Exception as e:
        if not st.session_state.preprocess_done:
            st.warning("âš  Train ë°ì´í„°ê°€ ì¡´ì¬í•˜ì§€ ì•Šê±°ë‚˜ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    if st.session_state.saved_params is not None:
        current_params = {
            "Data Shape": "" if st.session_state.get("df_train_preview") is None else str(st.session_state.get("df_train_preview").shape),
            "Train Task": "" if st.session_state.get("task_name_train") is None else str(st.session_state.get("task_name_train")),
            "Test Task": "" if st.session_state.get("test_task_fault") is None else str(st.session_state.get("test_task_fault")),
            "Feature Type": "" if st.session_state.get("feature_type") is None else str(st.session_state.get("feature_type")),
            "GVW": "" if st.session_state.get("gvw") is None else str(st.session_state.get("gvw")),
            "Drive Pattern": "" if st.session_state.get("drive_pattern") is None else str(st.session_state.get("drive_pattern")),
            "Min Length": "" if st.session_state.get("common_min_length") is None else str(st.session_state.get("common_min_length")),
            "Engine Threshold": "" if st.session_state.get("engine_threshold") is None else str(st.session_state.get("engine_threshold")),
            "Engine Threshold High": "" if st.session_state.get("engine_threshold_high") is None else str(st.session_state.get("engine_threshold_high")),
            "Fuel Threshold": "" if st.session_state.get("fuel_threshold") is None else str(st.session_state.get("fuel_threshold")),}
        diffs = []
        for k, saved_v in st.session_state.saved_params.items():
            curr_v = current_params.get(k, "")
            if str(saved_v) != str(curr_v):
                diffs.append(f"{k}: {saved_v} â†’ {curr_v}")

    df_params = None
    data_params = None
    if "df_params" in st.session_state and "data_params" in st.session_state:
        df_params = st.session_state.df_params
        data_params = st.session_state.data_params

    if "train_init" not in st.session_state:
        st.session_state.train_init = False
    if "train_ready" not in st.session_state:
        st.session_state.train_ready = False
    if "train_logs" not in st.session_state:
        st.session_state.train_logs = ""
    if "train_run_done" not in st.session_state:
        st.session_state.train_run_done = False
    if "latest_train_ckpt_dir" not in st.session_state:
        st.session_state.latest_train_ckpt_dir = None
    
    st.divider()
    col1, col2 = st.columns([15, 1])  # ë¹„ìœ¨ë¡œ ë„ˆë¹„ ì¡°ì • ê°€ëŠ¥
    with col1:
        st.markdown("#### â–¶ï¸ Deep Learningê¸°ë°˜ ì´ìƒíƒì§€ í•™ìŠµëª¨ë¸ êµ¬ì¶• (TranAD)")
    with col2:
        st.button("?",help="TranADëŠ” ì •ìƒ ì—”ì§„ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í•™ìŠµí•˜ê³ , ë™ì¼ ì—”ì§„ì˜ ë¶ˆëŸ‰ ë°ì´í„°ë¥¼ í†µí•´ ì´ìƒ íƒì§€ ì„±ëŠ¥ì„ ê²€ì¦Â·í–¥ìƒì‹œí‚¤ëŠ” Transformer ê¸°ë°˜ ì´ìƒíƒì§€ ëª¨ë¸ì…ë‹ˆë‹¤.")
    if not st.session_state.train_init:
        if st.button("â–¶ ì‹œì‘"):
            st.success("ë°ì´í„° ì „ì²˜ë¦¬ í™•ì¸ ì™„ë£Œ. í•™ìŠµì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            try:
                train_task_value = df_params.loc[df_params['Parameter'] == 'Train Task', 'Value'].iloc[0]
                if train_task_value is not None and len(diffs) == 0:
                    st.session_state.train_init = True
                    st.session_state.train_ready = False
                    st.success("âœ… í•™ìŠµëª¨ë¸ ë‹¨ê³„ ì´ë™ ì„±ê³µ (ì„¤ì •ê°’ ë³€ê²½ ì‹œ ì „ì²˜ë¦¬ ì¬ì‹¤í–‰ í•„ìš”)")
                    st.experimental_rerun() 
                else:
                    st.warning("âš  í•™ìŠµëª¨ë¸ ë‹¨ê³„ë¡œ ì´ë™ ë¶ˆê°€ (ì „ì²˜ë¦¬ ë³€ê²½ë¨)")
            except Exception as e:
                st.error(f"âš  Train Task í™•ì¸ ì˜¤ë¥˜: {e}")
        st.stop()   
    

    # ================================================================
    # 1ï¸âƒ£ STEP 1: TranAD íŒŒë¼ë¯¸í„° ì„¤ì • ë‹¨ê³„
    # ================================================================
    epoch, window_size, batch_size, learning_rate = activate_model_parameter_sidebar()
    param_toggle = "model_params_area_open"

    col1, col2 = st.columns([15, 1])  # ë¹„ìœ¨ë¡œ ë„ˆë¹„ ì¡°ì • ê°€ëŠ¥
    with col1:
        if st.button("ğŸ“Œ ëª¨ë¸ íŒŒë¼ë¯¸í„° ì„¤ì •"):
            st.session_state[param_toggle] = not st.session_state.get(param_toggle, False)
            st.session_state.train_ready = True
    with col2:
        st.button("?", help="í•™ìŠµëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ëŠ” Sidebar ë©”ë‰´ì˜ ğŸ“Œ TranAD íŒŒë¼ë¯¸í„° ì„¤ì •ì„ í†µí•´ì„œ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    if st.session_state.get(param_toggle, False):
        st.write("âœ… í•™ìŠµ íŒŒë¼ë¯¸í„° ì„¤ì • ì™„ë£Œ")
        if all(v is not None for v in [epoch, window_size, batch_size, learning_rate]):
            model_params = {"Epoch": epoch,"Window Size": window_size,"Batch Size": batch_size,"Learning Rate": learning_rate}
            df_model_params = pd.DataFrame(model_params.items(), columns=["Parameter", "Value"])
            st.table(df_model_params)
            st.success("âœ… íŒŒë¼ë¯¸í„° ë³€ê²½ ì‹œ ëª¨ë¸ ì¬í•™ìŠµ í•„ìˆ˜")

    # ================================================================
    # 2ï¸âƒ£ STEP 2: Eval ì—…ë¡œë“œ ë° í•™ìŠµ ì‹¤í–‰ ë‹¨ê³„
    # ================================================================
    col1, col2 = st.columns([15, 1])  # ë¹„ìœ¨ë¡œ ë„ˆë¹„ ì¡°ì • ê°€ëŠ¥
    with col1:
        df_eval_fault, eval_save_dir = data_upload_ui(st,"Eval","eval_fault",mode="eval",task_name=st.session_state.get("test_task_fault"),module=module)
    with col2:
        st.button("?", help="ì—”ì§„ ì´ìƒíƒì§€ ê²€ì¦ ëŒ€ìƒì¸ ë°ì´í„°ì— ëŒ€í•œ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. \n\n í•´ë‹¹ ê²€ì¦ìš© ì´ìƒë°ì´í„°ëŠ” í•™ìŠµëª¨ë¸ì˜ ì´ìƒíƒì§€ ëŠ¥ë ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.")

    if df_eval_fault is not None:
        st.session_state["df_eval_preview"] = df_eval_fault
    if eval_save_dir is not None:
        st.session_state["eval_save_dir"] = eval_save_dir
    try:
        eval_path_dir = st.session_state.get("eval_save_dir")
        if eval_path_dir:
            eval_csv = os.path.join(eval_path_dir, "data.csv")
            eval_column_list_path = os.path.join(eval_path_dir, "column_list.txt")
            st.session_state["eval_csv_path"] = eval_csv
            st.session_state["eval_column_list"] = eval_column_list_path
        else:
            eval_csv = st.session_state.get("eval_csv_path")
            eval_column_list_path = st.session_state.get("eval_column_list")
        df_eval_preview = st.session_state.get("df_eval_preview")
        if df_eval_preview is not None and eval_csv:
            with open(eval_column_list_path, "w", encoding="utf-8") as f:
                for col in df_eval_preview.columns:
                    if col not in ['file_number','attack','original_index','segment_original_start','segment_original_end']:
                        f.write(f"{col}\n")
            st.success(f"âœ… Eval ë°ì´í„° ì €ì¥ ì™„ë£Œ: {eval_csv}")
    except Exception as e:
        st.error(f"âš  Eval ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    # ë²„íŠ¼ í‘œì‹œ ì¡°ê±´
    eval_ready = (
        st.session_state.get("df_eval_preview") is not None and
        st.session_state.get("eval_csv_path") is not None and
        st.session_state.get("eval_column_list") is not None)
    if eval_ready:
        col1, col2 = st.columns([15, 1])  # ë¹„ìœ¨ë¡œ ë„ˆë¹„ ì¡°ì • ê°€ëŠ¥
        with col1:
            if st.button("ğŸš€ TranAD í•™ìŠµ"):
                train_file = pd.read_csv(os.path.join(train_path_dir, 'data.csv'), encoding='utf-8')
                eval_file = pd.read_csv(os.path.join(eval_path_dir, 'data.csv'), encoding='utf-8')
                if "TB" in task_name_test or "turbo" in task_name_test:
                    main_task = "Turbo_problem"
                elif "egr_sw" in task_name_test:
                    main_task = "EGR_SW"
                elif "egr_hw" in task_name_test:
                    main_task = "EGR_HW"
                else:
                    main_task = task_name_test
                result_path = os.path.join(BASE_DIR, "result", main_task,f"{main_task}_{feature_type}_gvw_{gvw}_{drive_pattern}_{task_name_test}_(Train)")
                os.makedirs(result_path, exist_ok=True)
                log_path = os.path.join(BASE_DIR, "logs")
                os.makedirs(log_path, exist_ok=True)
                log_file = os.path.join(log_path,f"{main_task}_{feature_type}_gvw_{gvw}_{drive_pattern}_{task_name_test}.txt")

                st.session_state['train_path_dir'] = train_path_dir
                st.session_state['eval_path_dir'] = eval_path_dir
                cmd = (
                    f"python main.py --phase train "
                    f"--model TranAD "
                    f"--task_name \"{task_name_test}\" "
                    f"--train_dataset \"{os.path.join(train_path_dir, 'data.csv')}\" "
                    f"--eval_dataset \"{os.path.join(eval_path_dir, 'data.csv')}\" "
                    f"--columns \"{column_list_path}\" "
                    f"--epoch {st.session_state.epoch} "
                    f"--window_size {st.session_state.window_size} "
                    f"--batch_size {st.session_state.batch_size} "
                    f"--learning_rate {st.session_state.learning_rate} "
                    f"--save_result_dir \"{result_path}\" ")
                
                env = os.environ.copy()
                env["CUDA_VISIBLE_DEVICES"] = "0"
                with st.spinner("í•™ìŠµ ì§„í–‰ ì¤‘..."):
                    rc, logs = run_subprocess_and_stream(cmd, env=env)
                st.session_state.train_logs = logs
                st.session_state.train_run_done = (rc == 0)
                try:
                    with open(log_file, "w", encoding="utf-8") as f:
                        f.write(logs)
                    st.success(f"í•™ìŠµ ë¡œê·¸ ì €ì¥ë¨: {log_file}")
                except:
                    st.error("ë¡œê·¸ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨")
                if rc == 0:
                    ckpt = find_latest_train_checkpoint(result_path, task_name_test)
                    if ckpt:
                        ckpt_dir = os.path.dirname(ckpt)
                        st.session_state.latest_train_ckpt_dir = ckpt
                        st.success(f"í•™ìŠµ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì™„ë£Œ: {ckpt}")
                    else:
                        st.error("ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. main.py ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                else:
                    st.error("í•™ìŠµ ì‹¤íŒ¨. ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

        with col2:
            st.button("?",help="í˜„ì¬ Project Management ë° ì „ì²˜ë¦¬ ì„¤ì •ì— ëŒ€í•œ í•™ìŠµëª¨ë¸ì„ êµ¬ì¶•í•œ ì´í›„, í•™ìŠµ ì™„ë£Œ í˜ì´ì§€ë¡œ ì „í™˜ë©ë‹ˆë‹¤.")


    # ---------------- Training Graph & Anomaly Score Plot ë‹¤ìš´ë¡œë“œ ----------------
    if st.session_state.train_run_done and st.session_state.get("latest_train_ckpt_dir"):
        st.text_area("í•™ìŠµ ë¡œê·¸", value=st.session_state.train_logs[-5000:], height=300)
        col1, col2 = st.columns(2)  
        pdf_path = os.path.join(os.path.dirname(st.session_state.latest_train_ckpt_dir),"train_loss_plot.pdf")
        with col1:
            if os.path.exists(pdf_path):
                with open(pdf_path, "rb") as f:
                    st.download_button(label="ğŸ“¥ Train Result Graph (New)",data=f.read(),file_name="train_loss_plot.pdf",mime="application/pdf",use_container_width=True)
        html_plot_path = os.path.join(os.path.dirname(st.session_state.latest_train_ckpt_dir),"anomaly_score.html")
        with col2:
            if os.path.exists(html_plot_path):
                with open(html_plot_path, "rb") as f:
                    st.download_button(label="ğŸ“¥ Train Anomaly Score Plot (New)",data=f.read(),file_name="train_result_plot.html",mime="text/html",use_container_width=True)
                    
        ##### ë°ì´í„° ì „ì²˜ë¦¬ & ëª¨ë¸ ê¸°ì´ˆ íŒŒë¼ë¯¸í„° ì •ë³´ Json ì €ì¥ 
        active_list = [data_params]
        try:
            save_path = os.path.join(BASE_DIR,result_path,task_name_test, "data_preprocessing_parameters.json")
            with open(save_path, "w", encoding="utf-8") as f:
                json.dump(active_list, f, indent=4)
            train_params = {"epoch": epoch,"window_size": window_size,"batch_size": batch_size,"learning_rate": learning_rate}
            result_base_dir = os.path.join(BASE_DIR,result_path,task_name_test)
            pattern = f"train_window_{st.session_state.window_size}_*"
            result_window_dirs = [
                d for d in glob(os.path.join(result_base_dir , pattern))
                if os.path.isdir(d)]
            latest_result_dir = max(result_window_dirs, key=os.path.getctime)
            dest_path = os.path.join(latest_result_dir,"data_preprocessing_parameters.json")
            shutil.move(save_path, dest_path)
           
            save_path = os.path.join(BASE_DIR, result_path, task_name_test, "model_training_parameters.json")
            with open(save_path, "w", encoding="utf-8") as f:
                json.dump(train_params, f, indent=4)
            result_base_dir = os.path.join(BASE_DIR,result_path,task_name_test)
            pattern = f"train_window_{st.session_state.window_size}_*"
            result_window_dirs = [
                d for d in glob(os.path.join(result_base_dir , pattern))
                if os.path.isdir(d)]
            latest_result_dir = max(result_window_dirs, key=os.path.getctime)
            dest_path = os.path.join(latest_result_dir,"model_training_parameters.json")
            shutil.move(save_path, dest_path)
            st.success("í•™ìŠµì„ ì„±ê³µì ìœ¼ë¡œ ë§ˆì³¤ìŠµë‹ˆë‹¤. í•™ìŠµ ê²°ê³¼ëŠ” Train Model Archiveë¡œ ì´ë™í•©ë‹ˆë‹¤.")

        except FileNotFoundError:
            st.warning("ğŸ“ ê²°ê³¼ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¬í•™ìŠµì„ ì‹¤í–‰í•˜ì„¸ìš”.")
        except ValueError:

            st.warning("âš ï¸ í•™ìŠµ ê²°ê³¼ ë””ë ‰í† ë¦¬ê°€ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì¬í•™ìŠµì„ ì‹¤í–‰í•˜ì„¸ìš”.")
        except Exception as e:
            print("ee")
            #st.warning(f"ì „ì²˜ë¦¬ ê³µí†µ ì„¤ì • ë° ì£¼í–‰íŒ¨í„´ ì„¸ë¶€ì‚¬í•­ì´ ë³€ê²½ëìŠµë‹ˆë‹¤. ì¬í•™ìŠµì„ ì‹¤í–‰í•˜ì„¸ìš”.")


def delete_other_folders(recall_list):
    st.markdown("ğŸ“ í•™ìŠµê²°ê³¼ í´ë” ì„ íƒ")
    folder_names = [item["folder"] for item in recall_list]
    selected = st.selectbox("í•™ìŠµ í´ë”ë¥¼ ì„ íƒí•˜ì„¸ìš” (ì„ íƒ ì•ˆí•  ì‹œ ê°€ì¥ ìµœê·¼ì— í•™ìŠµí•œ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤):", folder_names)
    if st.button("ğŸš€ ì„ íƒí•œ í´ë” ì œì™¸í•˜ê³  ë‚˜ë¨¸ì§€ ì‚­ì œ"):
        selected_item = next(item for item in recall_list if item["folder"] == selected)
        st.success(f"âœ… ì„ íƒëœ í´ë”ëŠ” ìœ ì§€ë©ë‹ˆë‹¤: {selected}")
        for item in recall_list:
            if item["folder"] != selected:
                folder_path = item["path"]
                try:
                    if os.path.exists(folder_path):
                        shutil.rmtree(folder_path)
                        st.warning(f"ğŸ§¹ ì‚­ì œë¨: {item['folder']} ({folder_path})")
                    else:
                        st.error(f"âŒ ê²½ë¡œ ì—†ìŒ: {folder_path}")
                except Exception as e:
                    st.error(f"âš  ì‚­ì œ ì‹¤íŒ¨ ({item['folder']}): {e}")
        st.success("ğŸ‰ ì‘ì—… ì™„ë£Œ! ì„ íƒëœ í´ë”ë§Œ ë‚¨ì•˜ìŠµë‹ˆë‹¤.")


def delete_incomplete_train_results(base_result_dir):
    train_window_dirs = [
        d for d in glob(os.path.join(base_result_dir, "train_window_*"))
        if os.path.isdir(d)]
    for d in train_window_dirs:
        result_pkl = os.path.join(d, "results_train.pkl")
        if not os.path.exists(result_pkl):
            try:
                shutil.rmtree(d)
            except Exception as e:
                st.error(f"âš  ì‚­ì œ ì‹¤íŒ¨ ({os.path.basename(d)}): {e}")
    st.success("âœ… ë¯¸ì™„ë£Œ í•™ìŠµ ê²°ê³¼ í´ë” ì‚­ì œ ì™„ë£Œ")


def update_model_data_params(ckpt):
    train_window_dir = os.path.basename(os.path.dirname(ckpt))
    parts = train_window_dir.split("_")
    window_size = int(parts[2])
    result_base_dir = os.path.join(BASE_DIR,result_path,task_name_test)
    pattern = f"train_window_{window_size}_*"
    train_window_dirs = [
        d for d in glob(os.path.join(result_base_dir, pattern))
        if os.path.isdir(d)]
    is_finetune = "finetune" in train_window_dir.lower()
    if is_finetune:
        train_window_dirs = [
            d for d in train_window_dirs
            if "finetune" in os.path.basename(d).lower()]
    else:
        train_window_dirs = [
            d for d in train_window_dirs
            if "finetune" not in os.path.basename(d).lower()]
    latest_result_dir = max(train_window_dirs, key=os.path.getctime)
    load_preprocessing_params = os.path.join(latest_result_dir,"data_preprocessing_parameters.json")
    load_model_params = os.path.join(latest_result_dir,"model_training_parameters.json")
    return latest_result_dir, load_preprocessing_params,load_model_params


def draw_footer(canvas, doc):
    canvas.saveState()
    canvas.setFont("HYSMyeongJo-Medium", 8)
    canvas.drawCentredString(A4[0] / 2, 10 * mm, f"- {doc.page} -")
    canvas.restoreState()

def load_summary_txt(page, base_dir):
    file_map = {
        1: "page_1_summary.txt",
        2: "page_2_summary.txt",
        3: "page_3_summary.txt",}
    file_name = file_map.get(page)
    if file_name is None:
        return ""
    file_path = os.path.join(base_dir, "summaries", file_name)
    if not os.path.exists(file_path):
        return f"[ERROR] Summary file not found: {file_name}"
    with open(file_path, "r", encoding="utf-8") as f:
        return f.read()

    
def generate_summary_pdf(summary_text):
    buffer = io.BytesIO()
    doc = SimpleDocTemplate(buffer,pagesize=A4,rightMargin=20 * mm,leftMargin=20 * mm,topMargin=20 * mm,bottomMargin=20 * mm)
    styles = getSampleStyleSheet()
    # ìµœìƒë‹¨ ëŒ€ì œëª© ìŠ¤íƒ€ì¼
    title_style = ParagraphStyle(name="KoreanTitle",parent=styles["Normal"],fontName="HYSMyeongJo-Medium",fontSize=16,leading=22,spaceAfter=16,spaceBefore=4,)
    # ë³¸ë¬¸ ìŠ¤íƒ€ì¼
    body_style = ParagraphStyle(name="KoreanBody",parent=styles["Normal"],fontName="HYSMyeongJo-Medium",fontSize=10,leading=14,spaceAfter=6)
    # 1~6ë²ˆ ì œëª© ìŠ¤íƒ€ì¼
    section_style = ParagraphStyle(name="KoreanSection",parent=styles["Normal"],fontName="HYSMyeongJo-Medium",fontSize=13,leading=18,spaceBefore=12,spaceAfter=8,)
    story = []
    for line in summary_text.split("\n"):
        line = line.strip()
        if not line:
            story.append(Spacer(1, 8))
            continue
        # âœ… ìµœìƒë‹¨ ëŒ€ì œëª©
        if line.startswith("[") and line.endswith("]"):
            story.append(Paragraph(line, title_style))
            story.append(Spacer(1, 6))
            continue
        # 1. 2. 3. ... ì„¹ì…˜ ì œëª© íŒë³„
        if (
            line.startswith("1.") or
            line.startswith("2.") or
            line.startswith("3.") or
            line.startswith("4.") or
            line.startswith("5.") or
            line.startswith("6.") or 
            line.startswith("7.")):
            story.append(Paragraph(line, section_style))
        else:
            story.append(Paragraph(line, body_style))
    doc.build(story,onFirstPage=draw_footer,onLaterPages=draw_footer)
    buffer.seek(0)
    return buffer.getvalue()




##################################################################################################################################################################################################################
sys.stdout = io.TextIOWrapper(sys.stdout.detach(), encoding='utf-8')
sys.stderr = io.TextIOWrapper(sys.stderr.detach(), encoding='utf-8')
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

def has_korean(path: str) -> bool:
    return bool(re.search(r"[ã„±-ã…ã…-ã…£ê°€-í£]", path))
if has_korean(BASE_DIR):
    st.warning(
        "âš ï¸ ì‹¤í–‰ ê²½ë¡œì— í•œê¸€ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n\n"
        "í˜„ì¬ ê²½ë¡œ:\n"
        f"`{BASE_DIR}`\n\n"
        "ëª¨ë¸ í•™ìŠµ ë° torch.save ê³¼ì •ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ\n"
        "**ì „ì²´ ì˜ë¬¸ ê²½ë¡œë¡œ ì´ë™ í˜¹ì€ ë³€ê²½ í›„ ë‹¤ì‹œ ì‹¤í–‰í•˜ì„¸ìš”.**")
    st.stop()
os.chdir(BASE_DIR)
sys.path.append(BASE_DIR)


current_task = st.session_state.get("task_select_ui", "TB_fault")
st.sidebar.markdown(f"# ğŸ—‚ï¸ Project Management : {current_task}") #ğŸ“
with st.sidebar.expander("âš™ï¸ ì—”ì§„ ì´ìƒíƒì§€ ë¬¸ì œ ì •ì˜", expanded=False):
    TASK_FILE = os.path.join(BASE_DIR, "task_list.txt")
    DEFAULT_TASKS = ["turbo_sw_per2","turbo_sw_per4","TB_fault","TB_gasket_problem","TB_fouling","egr_sw","egr_hw_per50","egr_hw_per90"]
    NEW_TASK_LABEL = "ìƒˆë¡œìš´ Task..."
    # ---------------- Default Reset ----------------
    if st.button("ğŸ”„ Task Reset"):
        with open(TASK_FILE, "w", encoding="utf-8") as f:
            for task in DEFAULT_TASKS:
                f.write(task + "\n")
        st.session_state.task_options = DEFAULT_TASKS + [NEW_TASK_LABEL]
        st.session_state.task_select_ui = "TB_fault"
        st.session_state.test_task_fault = "TB_fault"
        st.session_state.test_task_calib = "TB_fault"
        st.session_state.new_task = ""

    if not os.path.exists(TASK_FILE):
        with open(TASK_FILE, "w", encoding="utf-8") as f:
            for task in DEFAULT_TASKS:
                f.write(task + "\n")
    if "task_options" not in st.session_state:
        with open(TASK_FILE, "r", encoding="utf-8") as f:
            tasks = [line.strip() for line in f if line.strip()]
        if NEW_TASK_LABEL not in tasks:
            tasks.append(NEW_TASK_LABEL)
        st.session_state.task_options = tasks
        
    if "test_task_fault" not in st.session_state:
        st.session_state.test_task_fault = "TB_fault"
    if "task_select_ui" not in st.session_state:
        st.session_state.task_select_ui = st.session_state.test_task_fault

    st.selectbox("ğŸ“Œ Task ì„ íƒ (ì—”ì§„ ì´ìƒíƒì§€ ìœ í˜•)",options=st.session_state.task_options,key="task_select_ui",index=st.session_state.task_options.index(st.session_state.task_select_ui))
    if st.session_state.task_select_ui == NEW_TASK_LABEL:
        new_task = st.text_input("ìƒˆë¡œìš´ Task ì´ë¦„ì„ ì…ë ¥í•˜ì„¸ìš”",value=st.session_state.get("new_task", ""),key="new_task_input")
        if new_task and new_task not in st.session_state.task_options:
            st.session_state.task_options.insert(-1, new_task)
            with open(TASK_FILE, "w", encoding="utf-8") as f:
                for task in st.session_state.task_options:
                    if task != NEW_TASK_LABEL:
                        f.write(task + "\n")
            st.session_state.test_task_fault = new_task
            st.session_state.test_task_calib = new_task
            st.session_state.new_task = new_task      
    else:
        st.session_state.test_task_fault = st.session_state.task_select_ui
        st.session_state.test_task_calib = st.session_state.task_select_ui
        st.session_state.new_task = ""


st.sidebar.markdown("### âš™ï¸ App ì œì–´")
with st.sidebar.expander("Reset/Memory Clean ", expanded=False):
    if st.button("ğŸ§¹ Reset ( ì „ì²´ ì„¤ì • ì´ˆê¸°í™”)"):
        for key in list(st.session_state.keys()):
            del st.session_state[key]
        st.success("âœ… ì„¤ì • ì´ˆê¸°í™” ì™„ë£Œ. í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•©ë‹ˆë‹¤...")
        st.write('<meta http-equiv="refresh" content="0">', unsafe_allow_html=True)
    if st.button("ğŸ’¾ ë©”ëª¨ë¦¬ ì •ë¦¬"):
        st.success("âœ… ë©”ëª¨ë¦¬ ì •ë¦¬ ì™„ë£Œ")
        clean_memory()



st.sidebar.markdown("### âš™ï¸ Device ì„¤ì •")
with st.sidebar.expander("CPU/GPU Environment", expanded=False):
    device_status_placeholder = st.empty()
    device_option = st.radio("ğŸ’» Device ì„ íƒ",["GPU", "CPU"],index=0,key="device_option")
    if st.button("âš¡ Device ì ìš©"):
        if device_option == "GPU":
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            if device.type == "cuda":
                torch.cuda.empty_cache()
                device_status_placeholder.success("âš¡ GPU í™œì„±í™”")
            else:
                device_status_placeholder.warning("âŒ GPU ì‚¬ìš© ë¶ˆê°€ â†’ CPUë¡œ ì „í™˜")
        else:
            device = torch.device("cpu")
            gc.collect()
            device_status_placeholder.success("âš¡ CPU í™œì„±í™”")
        st.session_state.device = device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


st.sidebar.markdown("### âš™ï¸ í•µì‹¬ ê¸°ëŠ¥ ì„ íƒ")
page = st.sidebar.radio("## Page", ["Data Upload/Preprocess & Model Training","Anomaly Detection/Causal Analysis", "Trained Model Fine-Tuning"])
if "preprocessing_module" not in st.session_state:
    st.session_state.preprocessing_module = None
if "process" not in st.session_state:
    st.session_state.process = None
if "log_text" not in st.session_state:
    st.session_state.log_text = ""
if "progress" not in st.session_state:
    st.session_state.progress = 0.0


if page == "Data Upload/Preprocess & Model Training":
    top_left, top_right = st.columns([4, 1])
    with top_right:
        #pdf_bytes = generate_summary_pdf(get_page_summary(1))
        summary_text = load_summary_txt(page=1, base_dir=BASE_DIR)
        pdf_bytes = generate_summary_pdf(summary_text)
        st.download_button(label="ğŸ“„ê¸°ëŠ¥ ì„¤ëª…",data=pdf_bytes,file_name="ê¸°ëŠ¥ì„¤ëª…_Page1.pdf",mime="application/pdf",use_container_width=True)

    st.markdown(f"#### ğŸ“Š í•™ìŠµ ë°ì´í„° (ì •ìƒì—”ì§„) ì—…ë¡œë“œ & ì „ì²˜ë¦¬")
    with st.sidebar.expander("âš™ï¸ ê³µí†µ ì„¤ì •", expanded=False):
        if st.button("â–¶ï¸ ì „ì²˜ë¦¬ íŒ¨í‚¤ì§€ ì—…ë¡œë“œ"):
            status_text = st.empty()
            steps = ["íŒ¨í‚¤ì§€ ë¡œë”© ì¤€ë¹„", "ì „ì²˜ë¦¬ ëª¨ë“ˆ ë¡œë”©", "ê¸°ëŠ¥ ì¤€ë¹„ ì™„ë£Œ"]
            with st.spinner("ì „ì²˜ë¦¬ íŒ¨í‚¤ì§€ ë¡œë”© ì¤‘..."):
                for step in steps:
                    status_text.text(f"â³ {step} ...")
                    time.sleep(0.5)
                st.session_state.preprocessing_module = importlib.import_module("src.preprocessing")
                status_text.text("âœ… ì „ì²˜ë¦¬ ëª¨ë“ˆ ë¡œë”© ì™„ë£Œ")

        if "common_feature_type" not in st.session_state:
            st.session_state.common_feature_type = "full_feature"
        if "common_level" not in st.session_state:
            st.session_state.common_level = "Desc"
        if "drive_pattern" not in st.session_state:
            st.session_state.drive_pattern = "desc"
        if "common_gvw" not in st.session_state:
            st.session_state.common_gvw = 100
        if "engine_threshold_by_level" not in st.session_state:
            st.session_state.engine_threshold_by_level = {"Low": 1100,"Mid": 1100,"High": 1300,"Desc": 700}
        if "engine_threshold_high_by_level" not in st.session_state:
            st.session_state.engine_threshold_high_by_level = {"Mid": 1300}
        if "fuel_threshold_by_level" not in st.session_state:
            st.session_state.fuel_threshold_by_level = {"Low": 15.0,"Mid": 15.0,"High": 15.0,"Desc": 15.0}
        if "common_min_length_by_level" not in st.session_state:
            st.session_state.common_min_length_by_level = {"Low":60,"Mid":60,"High":60,"Desc":60}
        if "prev_level" not in st.session_state:
            st.session_state.prev_level = st.session_state.common_level
       
       
        def sync_feature_type():
            st.session_state.common_feature_type = st.session_state.feature_type_selectbox
        def sync_gvw():
            st.session_state.common_gvw = st.session_state.gvw_selectbox
        def sync_drive_pattern():
            st.session_state.common_level = st.session_state.drive_pattern_selectbox
            st.session_state.drive_pattern = st.session_state.common_level.lower()
        levels = ["Low", "Mid", "High", "Desc"]
        common_feature_type = st.selectbox("ğŸ“Œ Feature Type",["full_feature", "main_feature"],key="feature_type_selectbox",index=["full_feature","main_feature"].index(st.session_state.common_feature_type),on_change=sync_feature_type)
        common_gvw = st.selectbox("ğŸ“Œ GVW Level",[0,50,100],key="gvw_selectbox",index=[0,50,100].index(st.session_state.common_gvw),on_change=sync_gvw)
        selected_level = st.selectbox("ğŸ“Œ Drive Pattern ì„ íƒ",levels,key="drive_pattern_selectbox",index=levels.index(st.session_state.common_level),on_change=sync_drive_pattern)
        st.session_state.common_feature_type = common_feature_type 
        st.session_state.common_gvw = common_gvw
        st.session_state.common_level = selected_level
        st.session_state.drive_pattern = st.session_state.common_level.lower()

        with st.sidebar.expander("ğŸ”§ ì£¼í–‰íŒ¨í„´ ì„¸ë¶€ì‚¬í•­ ì„¤ì •", expanded=False):
            if st.session_state.prev_level != selected_level:
                prev = st.session_state.prev_level
                curr = selected_level
                st.session_state.engine_threshold_by_level[curr] = st.session_state.engine_threshold_by_level.get(prev, 0)
                st.session_state.fuel_threshold_by_level[curr] = st.session_state.fuel_threshold_by_level.get(prev, 15.0)
                st.session_state.common_min_length_by_level[curr] = st.session_state.common_min_length_by_level.get(prev, 60)
                if curr == "Mid":
                    st.session_state.engine_threshold_high_by_level["Mid"] = st.session_state.engine_threshold_high_by_level.get(prev, 1300)
                st.session_state.prev_level = curr

            engine_key = f"engine_{selected_level}"
            if engine_key not in st.session_state:
                st.session_state[engine_key] = st.session_state.engine_threshold_by_level[selected_level]
            engine_threshold = st.number_input(f"ğŸ“Œ {selected_level} Engine Speed Threshold",min_value=0,value=st.session_state[engine_key],step=1,key=engine_key,
                                                on_change=lambda: st.session_state.engine_threshold_by_level.update({selected_level: st.session_state[engine_key]}))

            if selected_level == "Mid":
                high_key = f"engine_high_{selected_level}"
                if high_key not in st.session_state:
                    st.session_state[high_key] = st.session_state.engine_threshold_high_by_level["Mid"]
                engine_threshold_high = st.number_input("ğŸ“Œ Mid Upper Engine Speed Threshold",min_value=0,value=st.session_state[high_key],step=1,
                    key=high_key,on_change=lambda: st.session_state.engine_threshold_high_by_level.update({"Mid": st.session_state[high_key]}))
            else:
                engine_threshold_high = None

            fuel_key = f"fuel_{selected_level}"
            if fuel_key not in st.session_state:
                st.session_state[fuel_key] = st.session_state.fuel_threshold_by_level[selected_level]
            fuel_threshold = st.number_input("ğŸ“Œ Fuel Threshold",min_value=0.0,value=st.session_state[fuel_key],step=0.1,key=fuel_key,
                                            on_change=lambda: st.session_state.fuel_threshold_by_level.update({selected_level: st.session_state[fuel_key]}))

            minlen_key = f"minlen_{selected_level}"
            if minlen_key not in st.session_state:
                st.session_state[minlen_key] = st.session_state.common_min_length_by_level[selected_level]
            common_min_length = st.number_input("ğŸ“Œ ìµœì†Œ ì—°ì† êµ¬ê°„ ê¸¸ì´",min_value=1,value=st.session_state[minlen_key],step=1,key=minlen_key,
                                    on_change=lambda: st.session_state.min_length_by_level.update({selected_level: st.session_state[minlen_key]}))

            st.session_state.engine_threshold = engine_threshold 
            st.session_state.engine_threshold_high = engine_threshold_high
            st.session_state.fuel_threshold = fuel_threshold
            st.session_state.common_min_length = common_min_length

            if st.button("ğŸ”„ í˜„ì¬ ì£¼í–‰íŒ¨í„´ ì„¤ì • ì´ˆê¸°í™”"):
                default_engine_threshold_map = {"Low": 1100,"Mid": 1100,"High": 1300,"Desc": 700}
                default_engine_threshold_high_map = {"Mid": 1300}
                default_engine_threshold = default_engine_threshold_map[selected_level]
                st.session_state.engine_threshold = default_engine_threshold
                st.session_state.engine_threshold_by_level[selected_level] = default_engine_threshold
                if selected_level == "Mid":
                    default_high = default_engine_threshold_high_map["Mid"]
                    st.session_state.engine_threshold_high = default_high
                    st.session_state.engine_threshold_high_by_level["Mid"] = default_high
                else:
                    st.session_state.engine_threshold_high = None
                st.session_state.fuel_threshold = 15.0
                st.session_state.common_min_length = 60
                st.success(f"âœ… {selected_level} ì£¼í–‰íŒ¨í„´ ì„¤ì •ì´ ê¸°ë³¸ê°’ìœ¼ë¡œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

                engine_threshold = default_engine_threshold 
                fuel_threshold = 15.0
                common_min_length = 60

                st.session_state.engine_threshold = engine_threshold
                st.session_state.fuel_threshold= fuel_threshold
                st.session_state.common_min_length = common_min_length

                st.write("ğŸ”½ ì£¼í–‰íŒ¨í„´ Reset ì™„ë£Œ")
                if selected_level== "Mid":
                    params_dict = {"engine_threshold": engine_threshold,"engine_threshold_high": engine_threshold_high, "fuel_threshold": fuel_threshold,"common_min_length": common_min_length}
                else:
                    params_dict = {"engine_threshold": engine_threshold,"fuel_threshold": fuel_threshold,"common_min_length": common_min_length}
                st.write(params_dict)


    
    # ---------- ì„¸ì…˜ ì´ˆê¸°í™” (ì²˜ìŒì— í•œ ë²ˆë§Œ) ----------
    if "preprocess_messages" not in st.session_state:
        st.session_state.preprocess_messages = []
    if "train_uploaded" not in st.session_state:
        st.session_state.train_uploaded = False
    if "df_train_preview" not in st.session_state:
        st.session_state.df_train_preview = None
    if "df_eval_preview" not in st.session_state:
        st.session_state.df_eval_preview = None
    if "train_save_dir" not in st.session_state:
        st.session_state.train_save_dir = None
    if "eval_save_dir" not in st.session_state:
        st.session_state.eval_save_dir = None
    if "saved_params" not in st.session_state:
        st.session_state.saved_params = None
    if "preprocess_done" not in st.session_state:
        st.session_state.preprocess_done = False
    if "params_synced" not in st.session_state:
        st.session_state.params_synced = False
    if "train_csv_path" not in st.session_state:
        st.session_state.train_csv_path = None
    if "eval_csv_path" not in st.session_state:
        st.session_state.eval_csv_path = None
    if "column_list" not in st.session_state:
        st.session_state.column_list = None


    # ---------- (ê¸°ì¡´) ë°ì´í„° ì—…ë¡œë“œ / ì „ì²˜ë¦¬ ì½”ë“œ ì‹¤í–‰ ----------
    module = st.session_state.get("preprocessing_module")
    if module is None:
        col1, col2 = st.columns([20, 1])
        with col1:
            st.info("âš™ï¸ ì „ì²˜ë¦¬ ëª¨ë“ˆ í™œì„±í™” í›„ ë°ì´í„° ì—…ë¡œë“œ ë° ì „ì²˜ë¦¬ ê°€ëŠ¥")
        with col2:
            st.button("!", help="ìƒˆë¡œê³ ì¹¨ ë° App ì¬ì‹¤í–‰ì„ í•  ë•Œ Sidebar ë©”ë‰´ì˜ âš™ï¸ ê³µí†µ ì„¤ì •ì—ì„œ â–¶ï¸ ì „ì²˜ë¦¬ íŒ¨í‚¤ì§€ ì—…ë¡œë“œë¥¼ ë°˜ë“œì‹œ ìˆ˜í–‰í•´ì•¼ í•©ë‹ˆë‹¤.")

    #### ê¸°ì¡´ í•™ìŠµ / ê²€ì¦ ì „ì²˜ë¦¬ ë°ì´í„° / í•™ìŠµëª¨ë¸ ì €ì¥ ì—¬ë¶€ í™•ì¸
    st.session_state["task_name_train"] = "Normal"
    task_name_train = st.session_state.get("task_name_train")
    task_name_test = st.session_state.get("test_task_fault")
    feature_type = st.session_state.common_feature_type
    gvw = st.session_state.common_gvw
    min_length = st.session_state.common_min_length
    fuel_threshold = st.session_state.fuel_threshold
    engine_threshold = st.session_state.engine_threshold
    engine_threshold_high = st.session_state.engine_threshold_high
    drive_pattern = st.session_state.get("drive_pattern")
    common_min_length = st.session_state.common_min_length
    st.session_state.feature_type = feature_type
    st.session_state.gvw = gvw
    
    train_path_dir = os.path.join(BASE_DIR,"data_preprocessed", f"normal_gvw{gvw}", feature_type, drive_pattern)
    eval_path_dir = os.path.join(BASE_DIR,"data_preprocessed",f"Eval_{task_name_test}",feature_type,drive_pattern)                          
    if "TB" in task_name_test or "turbo" in task_name_test:
        main_task = "Turbo_problem"
    elif "egr_sw" in task_name_test:
        main_task = "EGR_SW"
    elif "egr_hw" in task_name_test:
        main_task = "EGR_HW"
    else:
        main_task = task_name_test
    result_path = os.path.join(BASE_DIR, "result", main_task,f"{main_task}_{feature_type}_gvw_{gvw}_{drive_pattern}_{task_name_test}_(Train)")
    column_list_path = os.path.join(train_path_dir, "column_list.txt")
    ckpt_load = find_latest_train_checkpoint(result_path, task_name_test)

    if not ckpt_load or not os.path.exists(ckpt_load):
        if os.path.exists(result_path):
            shutil.rmtree(result_path)
            
    

   
    obliviate = False 
    paths_ok = (os.path.exists(train_path_dir) and os.path.exists(eval_path_dir)and os.path.exists(column_list_path) and os.path.exists(result_path) and os.path.exists(ckpt_load))
    if paths_ok:
        st.success("âœ… í˜„ì¬ âš™ï¸ ê³µí†µ ì„¤ì • ë° ğŸ”§ ì£¼í–‰íŒ¨í„´ ì„¸ë¶€ì‚¬í•­ ì„¤ì •ì— ëŒ€í•œ ì „ì²˜ë¦¬ ë°ì´í„° ë° í•™ìŠµ ê²°ê³¼ê°€ ì¡´ì¬í•©ë‹ˆë‹¤. (í•™ìŠµì™„ë£Œ í˜ì´ì§€ ì´ë™ ì™„ë£Œ)")
        #### 1.) ë°ì´í„° ì •ë³´ 
        latest_result_dir, load_preprocessing_params,load_model_params = update_model_data_params(ckpt_load)    

        df_train_normal = pd.read_csv(os.path.join(train_path_dir,"data.csv"),encoding='cp949')
        if df_train_normal is not None:
            st.session_state["df_train_preview"] = df_train_normal
        if train_path_dir is not None:
            st.session_state["train_save_dir"] = train_path_dir

        train_path_dir = st.session_state.get("train_save_dir")
        train_csv = os.path.join(train_path_dir, "data.csv")
        column_list_path = os.path.join(train_path_dir, "column_list.txt")
        st.session_state["train_csv_path"] = train_csv
        st.session_state["column_list"] = column_list_path
        
        df_train_preview = st.session_state.get("df_train_preview")
        with open(column_list_path, "w", encoding="utf-8") as f:
            for col in df_train_preview.columns:
                if col not in ['file_number','attack','original_index','segment_original_start','segment_original_end']:
                    f.write(f"{col}\n")
        st.session_state["df_train_preview"] = df_train_preview
        st.session_state["train_csv_path"] = train_csv
        st.session_state["column_list"] = column_list_path
        st.session_state.preprocess_done = True
        
        data_params = {
            "Data Shape": df_train_preview.shape,
            "Train Task": task_name_train,
            "Test Task": task_name_test,
            "Feature Type": feature_type,
            "GVW": gvw,
            "Drive Pattern": drive_pattern,
            "Min Length": common_min_length,
            "Engine Threshold": engine_threshold,
            "Engine Threshold High": engine_threshold_high,
            "Fuel Threshold": fuel_threshold}
        st.session_state.data_params = data_params

        norm_saved = {k: "" if v is None else str(v) for k, v in data_params.items()}
        st.session_state.saved_params = norm_saved
        df_params = pd.DataFrame([(k, v) for k, v in norm_saved.items()], columns=["Parameter", "Value"])
        st.session_state.df_params = df_params
        st.session_state.params_synced = True

        df_params = st.session_state.get("df_params")
        if st.session_state.params_synced:
            toggle_key = "df_params_toggle"
            default_toggle = st.session_state.get(toggle_key, True)
            col1, col2 = st.columns([15, 1])
            with col1:
                show_params = st.checkbox("ğŸ“Š Project Management / ì „ì²˜ë¦¬ ê³µí†µ ì„¤ì • ë° ì£¼í–‰íŒ¨í„´ ì„¸ë¶€ì‚¬í•­", value=default_toggle, key=toggle_key)
            with col2:
                st.button("?", help="âš™ï¸ ê³µí†µ ì„¤ì • / ğŸ”§ ì£¼í–‰íŒ¨í„´ ì„¸ë¶€ì‚¬í•­ ì „ì²˜ë¦¬ ì„¤ì •ì— ëŒ€í•œ ì €ì¥ëœ í•™ìŠµë°ì´í„°ê°€ ì¡´ì¬í•˜ë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.")
            if show_params:
                if df_params is not None and not df_params.empty:
                    rows, cols = df_train_preview.shape
                    st.markdown(
                        f"""
                        <div style="
                            padding:12px;
                            border-radius:10px;
                            font-size:13px;
                            color:inherit;">
                        
                        <b>ğŸ“ Project Management</b><br>
                        â€¢ ì—”ì§„ ì´ìƒíƒì§€ ìœ í˜•&nbsp;&nbsp;: {task_name_test}</code><br><br>

                        <hr style='border:2px solid #999;'>

                        <b>âš™ï¸ ê³µí†µ ì„¤ì •</b><br>
                        â€¢ Feature Type : {feature_type}</code><br>
                        â€¢ GVW : {gvw}</code><br>
                        â€¢ Drive Pattern : {drive_pattern}</code><br><br>
                        
                        <b>ğŸ”§ ì£¼í–‰íŒ¨í„´ ì„¸ë¶€ì‚¬í•­ ì„¤ì •</b><br>
                        â€¢ Engine Threshold:
                        {engine_threshold}</code> /
                        {engine_threshold_high}</code><br>
                        â€¢ Fuel Threshold: {fuel_threshold}</code><br>
                        â€¢ Min Length : {common_min_length}</code><br><br>

                        <b>ğŸ“Š í•™ìŠµ ë°ì´í„° í¬ê¸°</b><br>
                        â€¢ Data Shape&nbsp;&nbsp;: ({rows}, {cols})</code><br><br>
                        </div>
                        """,
                        unsafe_allow_html=True)
                    csv_data = df_train_preview.to_csv(index=False, encoding="utf-8-sig")
                    st.download_button(label="â¬‡ï¸ í•™ìŠµ ë°ì´í„° CSV ë‹¤ìš´ë¡œë“œ",data=csv_data,file_name="train_data.csv",mime="text/csv")
                    st.success("âœ… íŒŒë¼ë¯¸í„° ë³€ê²½ ì‹œ ì „ì²˜ë¦¬ ì¬ì‹¤í–‰ í•„ìˆ˜")
                    

        toggle_df_key = "df_train_preview_toggle"
        default_toggle_df = st.session_state.get(toggle_df_key, False)
        show_summary = st.checkbox("ğŸ“Š í•™ìŠµ ë°ì´í„° ìš”ì•½ í†µê³„ëŸ‰", value=default_toggle_df, key=toggle_df_key)
        if show_summary:
            df_summary = df_train_preview.drop(['file_number','original_index','segment_original_start','segment_original_end'],axis=1).describe()
            st.dataframe(df_summary, width=1200)  
            st.success("âœ… ìš”ì•½ í†µê³„ëŸ‰ í™•ì¸ ì™„ë£Œ")
            
        toggle_plot_key_pretrain = "df_pretrain_plot_toggle"
        default_toggle_plot_pretrain = st.session_state.get(toggle_plot_key_pretrain, False)
        show_plot = st.checkbox("ğŸ“ˆ í•™ìŠµ ë°ì´í„° ì‹œê°í™”", value=default_toggle_plot_pretrain, key=toggle_plot_key_pretrain)
        if show_plot:
            numeric_cols = df_train_preview.drop(['file_number','original_index','segment_original_start','segment_original_end'],axis=1).select_dtypes(include="number").columns.tolist()
            max_len = len(df_train_preview)
            if "range_idx" not in st.session_state:
                st.session_state.range_idx = (0, max_len)
                
            start_idx, end_idx = st.slider("Index Range",min_value=0,max_value=max_len,step=1,key="range_idx") 
            if start_idx > end_idx:
                st.session_state.range_idx = (end_idx, end_idx)
                start_idx, end_idx = end_idx, end_idx

            if "selected_multi_cols" not in st.session_state:
                st.session_state.selected_multi_cols = []

            multi_options = ["ğŸ”„ ì „ì²´ ë³€ìˆ˜ ì„ íƒ"] + numeric_cols
            selected_cols = st.multiselect("ğŸ“ˆ ì‹œê°í™”í•  ë³€ìˆ˜ë“¤ ì„ íƒ",multi_options,key="selected_multi_cols") 
            if "ğŸ”„ ì „ì²´ ë³€ìˆ˜ ì„ íƒ" in selected_cols:
                selected_cols = numeric_cols
            if selected_cols:
                with st.expander("ğŸ“Š Plot ê²°ê³¼ ë³´ê¸°", expanded=True):
                    for col in selected_cols:
                        sns.set(font_scale=1.0)
                        fig, ax = plt.subplots(figsize=(12, 5))
                        ax.plot(df_train_preview[col].iloc[start_idx:end_idx])
                        ax.set_title(f"{col} Plot (Index {start_idx} ~ {end_idx})")
                        ax.set_xlabel("Index")
                        ax.set_ylabel(col)
                        st.pyplot(fig)
                        buf = io.BytesIO()
                        fig.savefig(buf, format="png")
                        buf.seek(0)
                        st.download_button(label=f"ğŸ“¥ {col} Plot Download",data=buf,file_name=f"{col}_Train_plot.png",mime="image/png")

        df_params = None
        data_params = None
        if "df_params" in st.session_state and "data_params" in st.session_state:
            df_params = st.session_state.df_params
            data_params = st.session_state.data_params

     
        if os.path.exists(load_preprocessing_params) ==False:
            st.warning("âš ï¸ í•™ìŠµì„ ì¤‘ë‹¨í–ˆìŠµë‹ˆë‹¤.âš™ï¸ App ì œì–´ì—ì„œ ğŸ§¹ Reset ( ì „ì²´ ì„¤ì • ì´ˆê¸°í™”)ë¥¼ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
            undone_dir_path = os.path.dirname(load_preprocessing_params)
            shutil.rmtree(undone_dir_path)
            st.stop()

        if ckpt_load and load_preprocessing_params:
            with open(load_preprocessing_params, "r", encoding="utf-8") as f:
                loaded_json = json.load(f)
            loaded_params = loaded_json[0]
            differences = []
            for key in data_params:
                val_current = data_params.get(key)
                val_json = loaded_params.get(key)
                if key == "Data Shape" and isinstance(val_current, tuple):
                    val_current = list(val_current)
                if val_current != val_json:
                    differences.append({
                        "Parameter": key,
                        "Current Value": val_current,
                        "Trained Value": val_json})
                    
            if not differences:
                st.info("âœ… ê¸°ì¡´ í•™ìŠµ ë° í˜„ì¬ ì „ì²˜ë¦¬ ì„¤ì •ì´ ëª¨ë‘ ì¼ì¹˜í•©ë‹ˆë‹¤.")
                existing_ckpt_dir = os.path.dirname(os.path.dirname(ckpt_load))
                subfolders = [
                    os.path.join(existing_ckpt_dir, folder)
                    for folder in os.listdir(existing_ckpt_dir)
                    if os.path.isdir(os.path.join(existing_ckpt_dir, folder))]
                recall_list = []
                for folder in subfolders:
                    pkl_path = os.path.join(folder, "results_train.pkl")
                    if os.path.exists(pkl_path):
                        try:
                            with open(pkl_path, "rb") as f:
                                data = pickle.load(f)
                            recall_value = data.get("combined_metrics", {}).get("recall", None)
                            fpr_value = data.get("combined_metrics", {}).get("fpr", None)
                            thres_value = data.get("threshold",None)
                            recall_list.append({
                                "folder": os.path.basename(folder),
                                "path": folder,     # â˜… í•„ìˆ˜ â˜…
                                "recall": recall_value,
                                "FPR":fpr_value,
                                "Threshold":thres_value})
                        except Exception as e:
                            st.error(f"â— {folder} â†’ results_train.pkl ì½ê¸° ì‹¤íŒ¨: {e}")
                
                recall_list = sorted(recall_list,key=lambda x: ( x["recall"] is None,-(x["recall"] or 0),x["FPR"] is None,(x["FPR"] or float("inf"), x["Threshold"] is None,(x["Threshold"] or float("inf")))))
                base_result_dir = os.path.join(result_path, task_name_test)
                history_toggle_key = "trained_history_open"

                st.markdown("---")
                st.markdown("#### ğŸ—„ï¸ í•™ìŠµëª¨ë¸ ì´ë ¥")
                col1, col2 = st.columns([15, 1])  # ë¹„ìœ¨ë¡œ ë„ˆë¹„ ì¡°ì • ê°€ëŠ¥
                with col1:
                    if st.button("ğŸ“‚ Train Model Archive"):
                        st.session_state[history_toggle_key] = not st.session_state.get(history_toggle_key, False)
                with col2:
                    st.button("?", help="í˜„ì¬ ì „ì²˜ë¦¬/ì£¼í–‰íŒ¨í„´ ì„¤ì •ì— ëŒ€í•œ í•™ìŠµ ê²°ê³¼ ì´ë ¥ì…ë‹ˆë‹¤.")
                if st.session_state.get(history_toggle_key, False):
                    col1, col2 = st.columns([15, 1])  # ë¹„ìœ¨ë¡œ ë„ˆë¹„ ì¡°ì • ê°€ëŠ¥
                    with col1:
                        display_list = [{"Folder": item["folder"],"Recall": item["recall"],"FPR": item["FPR"] ,"Threshold": item["Threshold"]} for item in recall_list]
                        st.table(display_list)
                    with col2:
                        st.button("?",help="Recall : ì‹¤ì œ ì´ìƒ ì¤‘ ëª¨ë¸ì´ ì˜¬ë°”ë¥´ê²Œ íƒì§€í•œ ë¹„ìœ¨\n\n""FPR (False Positive Rate) : ì •ìƒ ì¤‘ ì´ìƒìœ¼ë¡œ ì˜ëª» íŒë‹¨í•œ ë¹„ìœ¨\n\n""Threshold : ì´ìƒ ì—¬ë¶€ë¥¼ íŒë‹¨í•˜ëŠ” ê¸°ì¤€ê°’ \n\n ì›í•˜ëŠ” ê¸°ì¤€ì„ í†µí•´ì„œ ì´ìƒíƒì§€ë¥¼ ìˆ˜í–‰í•  í•™ìŠµëª¨ë¸ì„ ì„ íƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

                    delete_other_folders(recall_list)
                    delete_incomplete_train_results(base_result_dir)

                    ckpt_update = find_latest_train_checkpoint(result_path, task_name_test)
                    st.session_state.latest_train_ckpt_dir = ckpt_update
                    latest_result_dir, load_preprocessing_params,load_model_params = update_model_data_params(ckpt_update)    
                    if st.session_state.get("latest_train_ckpt_dir"):      
                        st.success('âœ… Trained History Checkpoint ì¡´ì¬.\n\n í˜„ì¬ í•™ìŠµëœ ëª¨ë¸ë¡œ ì´ìƒíƒì§€ ìˆ˜í–‰ í˜¹ì€ ì¶”ê°€/ì¬í•™ìŠµì„ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤')
                        col1, col2 = st.columns(2)  
                        pdf_path = os.path.join(os.path.dirname(st.session_state.latest_train_ckpt_dir),"train_loss_plot.pdf")
                        with col1:
                            if os.path.exists(pdf_path):
                                with open(pdf_path, "rb") as f:
                                    st.download_button(label="ğŸ“¥ Train Result Graph (History)",data=f.read(),file_name="train_loss_plot.pdf",mime="application/pdf",use_container_width=True)
                        html_plot_path = os.path.join(os.path.dirname(st.session_state.latest_train_ckpt_dir),"anomaly_score.html")
                        with col2:
                            if os.path.exists(html_plot_path):
                                with open(html_plot_path, "rb") as f:
                                    st.download_button(label="ğŸ“¥ Train Anomaly Score Plot (History)",data=f.read(),file_name="train_result_plot.html",mime="text/html",use_container_width=True)
            else:
                st.error("âŒ ì£¼í–‰íŒ¨í„´ ì„¸ë¶€ì‚¬í•­ì´ ë³€ê²½ëìŠµë‹ˆë‹¤.")
                obliviate = True
                paths_ok = False

        
        if obliviate == False:
            toggle_key_retrain = "retrain_params_toggle"
            default_toggle_retrain = st.session_state.get(toggle_key_retrain, False)
            st.markdown("---")
            st.markdown("#### ğŸ› ï¸ ì¬í•™ìŠµ ì˜µì…˜")
            col1, col2 = st.columns([15, 1])  # ë¹„ìœ¨ë¡œ ë„ˆë¹„ ì¡°ì • ê°€ëŠ¥
            with col1:
                retrain_params = st.checkbox("ğŸ”„ ëª¨ë¸ íŒŒë¼ë¯¸í„° ì¬í•™ìŠµ",value=default_toggle_retrain,key=toggle_key_retrain)
            with col2:
                st.button("?", help="Sidebar ë©”ë‰´ì˜ ğŸ“Œ TranAD íŒŒë¼ë¯¸í„° ì„¤ì •ì„ í†µí•´ì„œ ì—…ë°ì´íŠ¸ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            if not retrain_params:
                with open(load_model_params, "r", encoding="utf-8") as f:
                    model_params = json.load(f)
                    epoch = model_params['epoch'] 
                    window_size= model_params['window_size']
                    batch_size= model_params['batch_size']
                    learning_rate = model_params['learning_rate']
                    st.session_state["epoch"] = epoch
                    st.session_state["window_size"] = window_size
                    st.session_state["batch_size"] = batch_size
                    st.session_state["learning_rate"] = learning_rate
                    
                toggle_key_open_params = "open_params_toggle"
                default_toggle_open_params = st.session_state.get(toggle_key_open_params, False)
                col1, col2 = st.columns([15, 1])  # ë¹„ìœ¨ë¡œ ë„ˆë¹„ ì¡°ì • ê°€ëŠ¥
                with col1:
                    open_params = st.checkbox("ğŸ“Š í˜„ì¬ í•™ìŠµ íŒŒë¼ë¯¸í„° ë³´ê¸°",value=default_toggle_open_params,key=toggle_key_open_params)
                with col2:
                    st.button("?", help="Sidebar ë©”ë‰´ì˜ ğŸ“Œ TranAD íŒŒë¼ë¯¸í„° ì„¤ì •ì„ í†µí•´ì„œ ì§€ì •í•œ ê°’ë“¤ì…ë‹ˆë‹¤.")
                if open_params:
                    model_params = {"Epoch": epoch,"Window Size": window_size,"Batch Size": batch_size,"Learning Rate": learning_rate}
                    df_model_params = pd.DataFrame(model_params.items(), columns=["Parameter", "Value"])
                    st.table(df_model_params)
                    st.success("âœ… íŒŒë¼ë¯¸í„° ë³€ê²½ ì‹œ ëª¨ë¸ ì¬í•™ìŠµ í•„ìˆ˜")

            else:
                epoch, window_size, batch_size, learning_rate = activate_model_parameter_sidebar()
                st.session_state.epoch = epoch
                st.session_state.window_size = window_size
                st.session_state.batch_size = batch_size
                st.session_state.learning_rate = learning_rate
                if "train_trigger" not in st.session_state:
                    st.session_state.train_trigger = False
                if "train_running" not in st.session_state:
                    st.session_state.train_running = False
                if "train_run_done" not in st.session_state:
                    st.session_state.train_run_done = False
                if "train_logs" not in st.session_state:
                    st.session_state.train_logs = ""
                if "latest_train_ckpt_dir" not in st.session_state:
                    st.session_state.latest_train_ckpt_dir = None

                toggle_key_open_params_update = "open_params_toggle_update"
                col1, col2 = st.columns([15, 1])  # ë¹„ìœ¨ë¡œ ë„ˆë¹„ ì¡°ì • ê°€ëŠ¥
                with col1:
                    open_params_update = st.checkbox("ğŸ“Š í•™ìŠµ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸",value=st.session_state.get(toggle_key_open_params_update, False),key=toggle_key_open_params_update)
                with col2:
                    st.button("?", help="Sidebar ë©”ë‰´ì˜ ğŸ“Œ TranAD íŒŒë¼ë¯¸í„° ì„¤ì •ì„ í†µí•´ì„œ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. ")

                if open_params_update:
                    model_params = {"Epoch": epoch,"Window Size": window_size,"Batch Size": batch_size,"Learning Rate": learning_rate}
                    st.table(pd.DataFrame(model_params.items(), columns=["Parameter", "Value"]))

                toggle_key_retrain_params = "model_params_toggle_update"

                col1, col2 = st.columns([15, 1])  # ë¹„ìœ¨ë¡œ ë„ˆë¹„ ì¡°ì • ê°€ëŠ¥
                with col1:
                    model_params_update = st.checkbox("ğŸš€ ëª¨ë¸ ì¬í•™ìŠµ ì‹¤í–‰",value=st.session_state.get(toggle_key_retrain_params, False),key=toggle_key_retrain_params)
                with col2:
                    st.button("?", help="ì—…ë°ì´íŠ¸í•œ íŒŒë¼ë¯¸í„°ë¥¼ í† ëŒ€ë¡œ ëª¨ë¸ì„ ì¬í•™ìŠµí•©ë‹ˆë‹¤. í˜„ì¬ í‘œì‹œë˜ëŠ” LogëŠ” ìµœê·¼ì— í•™ìŠµí•œ ê²°ê³¼ì…ë‹ˆë‹¤.")
                if model_params_update:
                    st.info("íŒŒë¼ë¯¸í„° ì¬í•™ìŠµ ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì¬í•™ìŠµì„ ì‹¤í–‰í•˜ë©´ ì•„ë˜ì˜ ê²°ê³¼ê°€ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤.")
                    if st.button("â–¶ï¸ ì¬í•™ìŠµ ì‹œì‘"):
                        st.session_state.train_trigger = True
                        st.session_state.train_run_done = False
                        st.session_state.train_logs = ""

                if st.session_state.train_trigger and not st.session_state.train_running:
                    st.session_state.train_running = True
                    st.session_state.train_trigger = False

                if st.session_state.train_running:
                    if "TB" in task_name_test or "turbo" in task_name_test:
                        main_task = "Turbo_problem"
                    elif "egr_sw" in task_name_test:
                        main_task = "EGR_SW"
                    elif "egr_hw" in task_name_test:
                        main_task = "EGR_HW"
                    else:
                        main_task = task_name_test
                    result_path = os.path.join(BASE_DIR, "result", main_task,f"{main_task}_{feature_type}_gvw_{gvw}_{drive_pattern}_{task_name_test}_(Train)")
                    os.makedirs(result_path, exist_ok=True)
                    log_path = os.path.join(BASE_DIR, "logs")
                    os.makedirs(log_path, exist_ok=True)
                    log_file = os.path.join(log_path,f"{main_task}_{feature_type}_gvw_{gvw}_{drive_pattern}_{task_name_test}.txt")
                    cmd = (
                        f"python main.py --phase train "
                        f"--model TranAD "
                        f"--task_name \"{task_name_test}\" "
                        f"--train_dataset \"{os.path.join(train_path_dir, 'data.csv')}\" "
                        f"--eval_dataset \"{os.path.join(eval_path_dir, 'data.csv')}\" "
                        f"--columns \"{column_list_path}\" "
                        f"--epoch {epoch} "
                        f"--window_size {window_size} "
                        f"--batch_size {batch_size} "
                        f"--learning_rate {learning_rate} "
                        f"--save_result_dir \"{result_path}\" ")

                    env = os.environ.copy()
                    env["CUDA_VISIBLE_DEVICES"] = "0"
                    with st.spinner("â³ ëª¨ë¸ í•™ìŠµ ì§„í–‰ ì¤‘..."):
                        rc, logs = run_subprocess_and_stream(cmd, env=env)
                    st.session_state.train_logs = logs
                    st.session_state.train_run_done = (rc == 0)
                    st.session_state.train_running = False
                    try:
                        with open(log_file, "w", encoding="utf-8") as f:
                            f.write(logs)
                    except:
                        pass
                    if rc == 0:
                        ckpt = find_latest_train_checkpoint(result_path, task_name_test)
                        if ckpt:
                            st.session_state.latest_train_ckpt_dir = ckpt

                if st.session_state.train_run_done:
                    is_finetune = (
                        st.session_state.latest_train_ckpt_dir is not None
                        and isinstance(st.session_state.latest_train_ckpt_dir, str)
                        and "finetune" in st.session_state.latest_train_ckpt_dir.lower()
                        and os.path.isfile(st.session_state.latest_train_ckpt_dir))
                    
                    logs_dir_finetuned = os.path.join(BASE_DIR, "logs")
                    finetuned_txt_files = [
                        f for f in os.listdir(logs_dir_finetuned)
                        if "finetuned" in f.lower()
                        and f.lower().endswith(".txt")
                        and os.path.isfile(os.path.join(logs_dir_finetuned, f))]
                    latest_finetuned_txt = None
                    latest_finetuned_content = ""

                    if finetuned_txt_files and is_finetune==True: 
                        latest_finetuned_txt = max(finetuned_txt_files,key=lambda f: os.path.getmtime(os.path.join(logs_dir_finetuned, f)))
                        txt_path = os.path.join(logs_dir_finetuned, latest_finetuned_txt)
                        with open(txt_path, "r", encoding="utf-8", errors="ignore") as f:
                            latest_finetuned_content = f.read()
                        st.text_area("í•™ìŠµ ë¡œê·¸ (Latest finetuned)",value=latest_finetuned_content[-5000:],height=300)
                    else:
                        st.text_area("í•™ìŠµ ë¡œê·¸",value=st.session_state.train_logs[-5000:],height=300)

                    if st.session_state.latest_train_ckpt_dir:
                        base_dir = os.path.dirname(st.session_state.latest_train_ckpt_dir) # í•­ìƒ ê°€ì¥ ìµœì‹  íŒŒì¼ ì¡ìŒ (finetune í¬í•¨)
                        col1, col2 = st.columns(2)
                        pdf_path = os.path.join(base_dir, "train_loss_plot.pdf")
                        with col1:
                            if os.path.exists(pdf_path):
                                with open(pdf_path, "rb") as f:
                                    st.download_button("ğŸ“¥ Train Result Graph (Updated)",f.read(),file_name="train_loss_plot.pdf",mime="application/pdf",use_container_width=True)
                        html_plot_path = os.path.join(base_dir, "anomaly_score.html")
                        with col2:
                            if os.path.exists(html_plot_path):
                                with open(html_plot_path, "rb") as f:
                                    st.download_button("ğŸ“¥ Train Anomaly Score Plot (Updated)",f.read(),file_name="train_result_plot.html",mime="text/html",use_container_width=True)

                        active_list = [data_params]
                        try:
                            save_path = os.path.join(BASE_DIR,result_path,task_name_test, "data_preprocessing_parameters.json")
                            with open(save_path, "w", encoding="utf-8") as f:
                                json.dump(active_list, f, indent=4)
                            train_params = {"epoch": epoch,"window_size": window_size,"batch_size": batch_size,"learning_rate": learning_rate}
                            result_base_dir = os.path.join(BASE_DIR,result_path,task_name_test)
                            pattern = f"train_window_{st.session_state.window_size}_*"
                            result_window_dirs = [
                                d for d in glob(os.path.join(result_base_dir , pattern))
                                if os.path.isdir(d)]
                            latest_result_dir = max(result_window_dirs, key=os.path.getctime)
                            dest_path = os.path.join(latest_result_dir,"data_preprocessing_parameters.json")
                            shutil.move(save_path, dest_path)
                         
                            save_path = os.path.join(BASE_DIR, result_path, task_name_test, "model_training_parameters.json")
                            with open(save_path, "w", encoding="utf-8") as f:
                                json.dump(train_params, f, indent=4)
                            result_base_dir = os.path.join(BASE_DIR,result_path,task_name_test)
                            pattern = f"train_window_{st.session_state.window_size}_*"
                            result_window_dirs = [
                                d for d in glob(os.path.join(result_base_dir , pattern))
                                if os.path.isdir(d)]
                            latest_result_dir = max(result_window_dirs, key=os.path.getctime)
                            dest_path = os.path.join(latest_result_dir,"model_training_parameters.json")
                            shutil.move(save_path, dest_path)

                        except FileNotFoundError:
                            st.warning("ğŸ“ ê²°ê³¼ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¬í•™ìŠµì„ ì‹¤í–‰í•˜ì„¸ìš”.")
                        except ValueError:
                            st.warning("âš ï¸ í•™ìŠµ ê²°ê³¼ ë””ë ‰í† ë¦¬ê°€ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì¬í•™ìŠµì„ ì‹¤í–‰í•˜ì„¸ìš”.")
                        except Exception as e:
                            st.error(f"âŒ íŒŒë¼ë¯¸í„° ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

            #---------------- ì „ì²´ ì¬í•™ìŠµ íŠ¸ë¦¬ê±° ----------------
            full_key_retrain = "retrain_full_toggle"
            col1, col2 = st.columns([15, 1])
            with col1:
                retrain_full = st.checkbox( "ğŸ”„ ì „ì²´ ì¬í•™ìŠµ",value=st.session_state.get(full_key_retrain, False),key=full_key_retrain)
            with col2:
                st.button("?",help=("ëª¨ë¸ íŒŒë¼ë¯¸í„° ì¬í•™ìŠµ ì°½ì´ ì—´ë ¤ ìˆì„ ì‹œ ë„ê³  ë‹¤ì‹œ ğŸ”„ ì „ì²´ ì¬í•™ìŠµì„ ìˆ˜í–‰í•˜ì„¸ìš”.\n\n""í•„ìš” ì‹œ ì§„í–‰ì„ ìœ„í•´ â–¶ï¸ TranAD ëª¨ë¸ í•™ìŠµ ì‹¤í–‰ í•˜ë‹¨ì˜ â–¶ ì‹œì‘ì„ ì—¬ëŸ¬ ë²ˆ ëˆŒëŸ¬ì£¼ì„¸ìš”."))
            if retrain_full:
                st.warning("âš ï¸ í•™ìŠµ/ê²€ì¦ ë°ì´í„° ì „ì²˜ë¦¬ ë° ëª¨ë¸ í•™ìŠµ ì „ ê³¼ì •ì„ ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì§„í–‰í•©ë‹ˆë‹¤.")
                st.session_state.force_full_retrain = True
                sync_model_params_from_ui()
                try:
                    train_full_run()
                except:
                     st.warning("íŒŒë¼ë¯¸í„° ì¬í•™ìŠµ ì°½ì´ ì—´ë ¤ ìˆì„ ì‹œ ë„ê³  ë‹¤ì‹œ ğŸ”„ ì „ì²´ ì¬í•™ìŠµì„ ìˆ˜í–‰í•˜ì„¸ìš”.")

        else:
            pass
        
    if paths_ok==False or obliviate==True:
        st.warning(
    "âŒ í˜„ì¬ ì„¤ì •ì— ëŒ€í•œ í•™ìŠµ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.\n\n"
    "--------------------------------------------------------------------------------------------------------------------------------------------------------\n\n"
    "1.) ìµœì´ˆ ì‘ì—… ì‹¤í–‰ ì‹œ:\n\n"
    "â€¢ ì „ ê³¼ì • ì‹¤ì‹œ : í•„ìˆ˜\n\n"
    
    "--------------------------------------------------------------------------------------------------------------------------------------------------------\n\n"
    "2.) Project Management ì„¤ì •ë§Œ ë³€ê²½ ì‹œ:\n\n"
    "â€¢ í•™ìŠµ ë°ì´í„° ì¬í•™ìŠµ: ì„ íƒ\n\n"
    "â€¢ ê²€ì¦ ë°ì´í„° ì¬ì—…ë¡œë“œ ë° ì „ì²˜ë¦¬ í›„ ì¬í•™ìŠµ: í•„ìˆ˜ \n\n"
    "--------------------------------------------------------------------------------------------------------------------------------------------------------\n\n"
    "3.) ê·¸ ì™¸ ì„¤ì • í•¨ê»˜ ë³€ê²½ ì‹œ:\n\n"
    "â€¢ í•™ìŠµ ë°ì´í„° ì¬í•™ìŠµ: í•„ìˆ˜\n\n"
    "â€¢ ê²€ì¦ ë°ì´í„° ì¬ì—…ë¡œë“œ ë° ì „ì²˜ë¦¬ í›„ ì¬í•™ìŠµ: í•„ìˆ˜")
        train_full_run()
        
    
#######################################################################################################################################################
#######################################################################################################################################################
#######################################################################################################################################################
#######################################################################################################################################################
#######################################################################################################################################################
#######################################################################################################################################################


if page == "Anomaly Detection/Causal Analysis":
    top_left, top_right = st.columns([4, 1])
    with top_right:
        #pdf_bytes = generate_summary_pdf(get_page_summary(2))
        summary_text = load_summary_txt(page=2, base_dir=BASE_DIR)
        pdf_bytes = generate_summary_pdf(summary_text)
        st.download_button(label="ğŸ“„ê¸°ëŠ¥ ì„¤ëª…",data=pdf_bytes,file_name="ê¸°ëŠ¥ì„¤ëª…_Page2.pdf",mime="application/pdf",use_container_width=True)

    st.markdown(f"#### ğŸ“‰ Anomaly Detection")
    if "df_test_preview" not in st.session_state:
        st.session_state.df_test_preview = None
    if "test_csv_path" not in st.session_state:
        st.session_state.test_csv_path = None
    if "test_save_dir" not in st.session_state:
        st.session_state.test_save_dir = None
    if "use_calibration" not in st.session_state:
        st.session_state.use_calibration = True
    if "df_calibration_preview" not in st.session_state:
        st.session_state.df_calibration_preview = None
    if "calibration_csv_path" not in st.session_state:
        st.session_state.calibration_csv_path = None
    if "calibration_save_dir" not in st.session_state:
        st.session_state.calibration_save_dir = None
    
    task_name_train = st.session_state.get("task_name_train")
    task_name_test = st.session_state.get("test_task_fault")
    feature_type = st.session_state.get("feature_type") or st.session_state.common_feature_type
    gvw = st.session_state.get("gvw") or st.session_state.common_gvw
    drive_pattern = st.session_state.get("drive_pattern")
    common_min_length = st.session_state.get("common_min_length") or st.session_state.common_min_length
    engine_threshold = st.session_state.get("engine_threshold") or st.session_state.engine_threshold
    engine_threshold_high = st.session_state.get("engine_threshold_high") or st.session_state.engine_threshold_high
    fuel_threshold = st.session_state.get("fuel_threshold")
    module = st.session_state.get("preprocessing_module")
    log_path = os.path.join(BASE_DIR, "logs")
    os.makedirs(log_path, exist_ok=True)

    if "TB" in task_name_test or "turbo" in task_name_test:
        main_task = "Turbo_problem"
    elif "egr_sw" in task_name_test:
        main_task = "EGR_SW"
    elif "egr_hw" in task_name_test:
        main_task = "EGR_HW"
    else:
        main_task = task_name_test
    result_path = os.path.join(BASE_DIR, "result", main_task,f"{main_task}_{feature_type}_gvw_{gvw}_{drive_pattern}_{task_name_test}_(Train)")
    ckpt_recent = find_latest_train_checkpoint(result_path, task_name_test)
    if ckpt_recent ==None:
        st.warning("âš ï¸ í•™ìŠµ ë° ì¶”ë¡ ì„ ìˆ˜í–‰í•  ì¤€ë¹„ê°€ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ .\n\n"
                   "ë¨¼ì € **Data Upload and Preprocess** í˜ì´ì§€ì˜ ì‘ì—…ì„ ì™„ì„±í•˜ì„¸ìš”.")
        st.stop()  

    required_keys = ['task_name_train','test_task_fault','feature_type','gvw','drive_pattern','common_min_length','engine_threshold','fuel_threshold']#['feature_type'] 
    missing = [k for k in required_keys if k not in st.session_state or st.session_state[k] in [None, ""]]
    if missing:
        st.warning("âš ï¸ í•™ìŠµ ë° ì¶”ë¡ ì„ ìˆ˜í–‰í•  ì¤€ë¹„ê°€ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ .\n\n"
                   "ë¨¼ì € **Data Upload and Preprocess** í˜ì´ì§€ì˜ ì‘ì—…ì„ ì™„ì„±í•˜ì„¸ìš”.")
        st.stop()   


    with st.expander("##### ğŸ“Š ì¶”ë¡  / ë³´ì • ë°ì´í„° ì¤€ë¹„", expanded=True): #
        col1, col2 = st.columns([15, 1])  # ë¹„ìœ¨ë¡œ ë„ˆë¹„ ì¡°ì • ê°€ëŠ¥
        with col1:
             df_test_fault, test_save_dir = data_upload_ui(st,"Test","test_fault",mode="test",task_name=st.session_state.get("test_task_fault"),module=module)
        with col2:
            st.button("?", help="ì—”ì§„ ì´ìƒíƒì§€ ì˜ˆì¸¡ ëŒ€ìƒì¸ ë°ì´í„°ì— ëŒ€í•œ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n\nì „ì²˜ë¦¬ëŠ” 1í˜ì´ì§€ì˜ âš™ï¸ê³µí†µ ì„¤ì •/ğŸ”§ì£¼í–‰íŒ¨í„´ ì„¸ë¶€ì‚¬í•­ ì„¤ì •ê°’ì„ í† ëŒ€ë¡œ ì§„í–‰ë©ë‹ˆë‹¤.")
        if df_test_fault is not None:
            st.session_state["df_test_preview"] = df_test_fault
        if test_save_dir is not None:
            st.session_state["test_save_dir"] = test_save_dir
        try:
            test_path_dir = st.session_state.get("test_save_dir")
            if test_path_dir:
                test_csv = os.path.join(test_path_dir, "data.csv")
                test_column_list_path = os.path.join(test_path_dir, "column_list.txt")
                st.session_state["test_csv_path"] = test_csv
                st.session_state["test_column_list"] = test_column_list_path
            else:
                test_csv = st.session_state.get("test_csv_path")
                test_column_list_path = st.session_state.get("test_column_list")
            df_test_preview = st.session_state.get("df_test_preview")
            if df_test_preview is not None and test_csv:
                with open(test_column_list_path, "w", encoding="utf-8") as f:
                    for col in df_test_preview.columns: # state rerun ëˆ„ì  ì •ë³´ ë°©ì§€
                        if col not in ['file_number','attack','original_index','segment_original_start','segment_original_end','prediction','anomaly_score','threshold']:
                            f.write(f"{col}\n")
                st.success(f"âœ… Test ë°ì´í„° ì €ì¥ ì™„ë£Œ: {test_csv}")
        except Exception as e:
            st.error(f"âš  Test ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

        # ë²„íŠ¼ í‘œì‹œ ì¡°ê±´
        test_ready = (
            st.session_state.get("df_test_preview") is not None and
            st.session_state.get("test_csv_path") is not None and
            st.session_state.get("test_column_list") is not None)
        if test_ready:
            use_checkbox = st.checkbox("ğŸ“Œ Normal Calibration ìˆ˜í–‰ ì—¬ë¶€", value=True)
            st.session_state["use_calibration"] = use_checkbox
            # --- Calibration ìˆ˜í–‰ ì¡°ê±´ ---
            if st.session_state["use_calibration"]:

                # 1) ê¸°ì¡´ Calibration íŒŒì¼ì´ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                calibration_csv_path = os.path.join(BASE_DIR, "data_preprocessed","normal_engine_B",feature_type,drive_pattern,"data.csv")
                calibration_save_dir = os.path.join(BASE_DIR, "data_preprocessed","normal_engine_B",feature_type,drive_pattern)
                calibration_exists = (calibration_csv_path is not None and os.path.exists(calibration_csv_path) and os.path.exists(calibration_save_dir))
                if calibration_exists:
                    reupload = st.checkbox("ğŸ“¤ Normal Calibration Data ìƒˆë¡œ ì—…ë¡œë“œí•˜ê¸°")
                    if not reupload:
                        st.success(f"ğŸ“ ê¸°ì¡´ Normal Calibration ë°ì´í„°ë¥¼ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤.\n: {calibration_csv_path}")
                        try:
                            calibration_csv = pd.read_csv(calibration_csv_path)
                            st.session_state["df_calibration_preview"] = calibration_csv
                            st.session_state.calibration_csv_path = calibration_csv_path
                            st.session_state.calibration_save_dir = calibration_save_dir 
                        except Exception as e:
                            st.error(f"âš  ê¸°ì¡´ Calibration ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜: {e}")
                    else:
                        col1, col2 = st.columns([15, 1])  # ë¹„ìœ¨ë¡œ ë„ˆë¹„ ì¡°ì • ê°€ëŠ¥
                        with col1:
                            df_normal_calibration, calib_save_dir = data_upload_ui(st,"Calibration","test_calibration",mode="normal_calibration",task_name=st.session_state.get("test_task_fault"),module=module)
                        with col2:
                            st.button("?", help="ë‹¤ë¥¸ ì—”ì§„ì˜ ì •ìƒë°ì´í„°ì— ëŒ€í•œ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n\n ì „ì²˜ë¦¬ëŠ” 1í˜ì´ì§€ì˜ âš™ï¸ê³µí†µ ì„¤ì •/ğŸ”§ì£¼í–‰íŒ¨í„´ ì„¸ë¶€ì‚¬í•­ ì„¤ì •ê°’ì„ í† ëŒ€ë¡œ ì§„í–‰ë©ë‹ˆë‹¤. \n\ní•´ë‹¹ ë°ì´í„°ëŠ” í•™ìŠµëª¨ë¸ì˜ ì˜¤íƒì§€ ìœ„í—˜ ì¤‘ ê³¼ê²€ë¥ ì„ ì™„í™”í•˜ëŠ” ë°ì— ì‚¬ìš©ë©ë‹ˆë‹¤.")
                        if df_normal_calibration is not None:
                            st.session_state["df_calibration_preview"] = df_normal_calibration
                        if calib_save_dir is not None:
                            st.session_state["calibration_save_dir"] = calib_save_dir
                        try:
                            calibration_path_dir = st.session_state.get("calibration_save_dir")
                            if calibration_path_dir:
                                calibration_csv = os.path.join(calibration_path_dir, "data.csv")
                                calibration_column_list_path = os.path.join(calibration_path_dir, "column_list.txt")
                                st.session_state["calibration_csv_path"] = calibration_csv
                                st.session_state["calibration_column_list"] = calibration_column_list_path
                            else:
                                calibration_csv = st.session_state.get("calibration_csv_path")
                                calibration_column_list_path = st.session_state.get("calibration_column_list")
                            df_calibration_preview = st.session_state.get("df_calibration_preview")
                            if df_calibration_preview is not None and calibration_csv:
                                with open(calibration_column_list_path, "w", encoding="utf-8") as f:
                                    for col in df_calibration_preview.columns:
                                        if col not in ['file_number','attack','original_index','segment_original_start','segment_original_end']:
                                            f.write(f"{col}\n")
                                st.success(f"âœ… New Calibration ë°ì´í„° ì €ì¥ ì™„ë£Œ: {calibration_csv}")
                        except Exception as e:
                            st.error(f"âš  Calibration ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                else:
                    col1, col2 = st.columns([15, 1])  # ë¹„ìœ¨ë¡œ ë„ˆë¹„ ì¡°ì • ê°€ëŠ¥
                    with col1:
                        df_normal_calibration, calib_save_dir = data_upload_ui(st,"Calibration","test_calibration",mode="normal_calibration",task_name=st.session_state.get("test_task_fault"),module=module)
                    with col2:
                        st.button("?", help="ë‹¤ë¥¸ ì—”ì§„ì˜ ì •ìƒë°ì´í„°ì— ëŒ€í•œ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n\ní•´ë‹¹ ë°ì´í„°ëŠ” í•™ìŠµëª¨ë¸ì˜ ì˜¤íƒì§€ ìœ„í—˜ ì¤‘ ê³¼ê²€ë¥ ì„ ì™„í™”í•˜ëŠ” ë°ì— ì‚¬ìš©ë©ë‹ˆë‹¤.")
                    if df_normal_calibration is not None:
                        st.session_state["df_calibration_preview"] = df_normal_calibration
                    if calib_save_dir is not None:
                        st.session_state["calibration_save_dir"] = calib_save_dir
                    try:
                        calibration_path_dir = st.session_state.get("calibration_save_dir")
                        if calibration_path_dir:
                            calibration_csv = os.path.join(calibration_path_dir, "data.csv")
                            calibration_column_list_path = os.path.join(calibration_path_dir, "column_list.txt")
                            st.session_state["calibration_csv_path"] = calibration_csv
                            st.session_state["calibration_column_list"] = calibration_column_list_path
                        else:
                            calibration_csv = st.session_state.get("calibration_csv_path")
                            calibration_column_list_path = st.session_state.get("calibration_column_list")
                        df_calibration_preview = st.session_state.get("df_calibration_preview")
                        if df_calibration_preview is not None and calibration_csv:
                            with open(calibration_column_list_path, "w", encoding="utf-8") as f:
                                for col in df_calibration_preview.columns:
                                    if col not in ['file_number','attack','original_index','segment_original_start','segment_original_end']:
                                        f.write(f"{col}\n")
                            st.success(f"âœ… Calibration ë°ì´í„° ì €ì¥ ì™„ë£Œ: {calibration_csv}")
                    except Exception as e:
                        st.error(f"âš  Calibration ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

                result_path_test = os.path.join(BASE_DIR, "result", main_task,f"Target_Inference_{main_task}_{feature_type}_gvw_{gvw}_{drive_pattern}_{task_name_test}")
                log_test_file = os.path.join(log_path,f"Target_Inference_{main_task}_{feature_type}_gvw_{gvw}_{drive_pattern}_{task_name_test}.txt")

            else:
                st.info("âš™ï¸ Calibration ì˜µì…˜ì´ êº¼ì ¸ ìˆìŠµë‹ˆë‹¤. Calibration ê³¼ì •ì„ ìƒëµí•©ë‹ˆë‹¤.")
                st.session_state.df_calibration_preview = None
                st.session_state.calibration_csv_path = None
                st.session_state.calibration_save_dir = None
                result_path_test = os.path.join(BASE_DIR, "result", main_task,f"Target_Inference_No_Calibration_{main_task}_{feature_type}_gvw_{gvw}_{drive_pattern}_{task_name_test}")
                log_test_file = os.path.join(log_path,f"Target_Inference_No_Calibration_{main_task}_{feature_type}_gvw_{gvw}_{drive_pattern}_{task_name_test}.txt")
            os.makedirs(result_path_test, exist_ok=True)
            st.session_state["result_path_test"] = result_path_test
            st.session_state["log_test_file"] = log_test_file

        result_path = os.path.join(BASE_DIR, "result", main_task,f"{main_task}_{feature_type}_gvw_{gvw}_{drive_pattern}_{task_name_test}_(Train)")
        ckpt_load = find_latest_train_checkpoint(result_path, task_name_test)
        st.session_state["latest_train_ckpt_dir"] = ckpt_load

        model_params_path = os.path.join(BASE_DIR, result_path, task_name_test, "model_training_parameters.json")
        if os.path.exists(model_params_path ):
            with open(model_params_path , "r", encoding="utf-8") as f:
                loaded_model_params = json.load(f)  
    
    with st.expander("##### â–¶ï¸ ì¶”ë¡  ì‹¤í–‰", expanded=True):
        col1, col2 = st.columns([15, 1])  # ë¹„ìœ¨ë¡œ ë„ˆë¹„ ì¡°ì • ê°€ëŠ¥
        with col1:
            if st.button("ğŸš€ TranAD ì¶”ë¡  ì‹¤í–‰"):
                test_csv = st.session_state.get("test_csv_path")
                test_column_list = st.session_state.get("column_list") 
                use_calibration = st.session_state.get("use_calibration")
                calibration_csv = st.session_state.get("calibration_csv_path")
                result_path_test = st.session_state.get("result_path_test")
                log_test_file = st.session_state.get("log_test_file")
                ckpt_dir = st.session_state.get("latest_train_ckpt_dir")
                window_size = st.session_state.get("window_size") or loaded_model_params.get("window_size")
                batch_size = st.session_state.get("batch_size") or loaded_model_params.get("batch_size")
                epoch = st.session_state.get("epoch") or loaded_model_params.get("epoch")
                st.session_state["window_size"] = window_size
                st.session_state["batch_size"] = batch_size
                st.session_state["epoch"] = epoch

                if not test_csv or not test_column_list:
                    st.error("âš ï¸ Test ë°ì´í„° ì¤€ë¹„ê°€ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                    st.stop()
                if not ckpt_dir:
                    st.error("âš ï¸ í•™ìŠµ ì²´í¬í¬ì¸íŠ¸ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¨¼ì € í•™ìŠµì„ ì™„ë£Œí•˜ì„¸ìš”.")
                    st.stop()
                cmd = (
                    f"python main.py --phase test "
                    f"--model TranAD "
                    f"--task_name \"{task_name_test}\" "
                    f"--test_dataset \"{test_csv}\" "
                    f"--columns \"{test_column_list}\" "
                    f"--window_size {window_size} "
                    f"--batch_size {batch_size} "
                    f"--save_result_dir \"{result_path_test}\" "
                    f"--model_path \"{ckpt_dir}\" ")
                if use_calibration and calibration_csv:
                    cmd += f" --calibration_normal_dataset \"{calibration_csv}\" "
                else:
                    st.write("Calibration ê³¼ì •ì„ ìƒëµí•©ë‹ˆë‹¤")
                env = os.environ.copy()
                env["CUDA_VISIBLE_DEVICES"] = "0"

                with st.spinner("ì¶”ë¡  ì§„í–‰ ì¤‘..."):
                    rc, logs = run_subprocess_and_stream(cmd, env=env)
                st.session_state.test_logs = logs
                st.session_state.test_run_done = (rc == 0)
                try:
                    with open(log_test_file, "w", encoding="utf-8") as f:
                        f.write(logs)
                    st.success(f"Inference ë¡œê·¸ ì €ì¥ë¨: {log_test_file}")
                except:
                    st.error("Inference ë¡œê·¸ ì €ì¥ ì‹¤íŒ¨")

                if rc == 0:
                    latest_result = find_latest_test_result(result_path_test, task_name_test)
                    if latest_result:
                        st.session_state.latest_test_result = latest_result
                        st.success(f"ì¶”ë¡  ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {latest_result}")
                    else:
                        st.error("ì¶”ë¡  ê²°ê³¼ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    st.error("ì¶”ë¡  ì‹¤íŒ¨. ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        with col2:
            st.button("!", help="ì¶”ë¡  ì´í›„ ê²°ê³¼ plotê³¼ csvíŒŒì¼ì„ ë‹¤ìš´ë°›ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.")

        if st.session_state.get("test_run_done") and st.session_state.get("latest_test_result"):
            st.text_area("ì¶”ë¡  ë¡œê·¸", value=st.session_state.test_logs[-5000:], height=300)
            html_plot_path = os.path.join(os.path.dirname(st.session_state.latest_test_result), f"{task_name_test}_anomaly_score_plotly.html")
            if os.path.exists(html_plot_path):
                with open(html_plot_path, "rb") as f:
                    st.download_button(
                        label="ğŸ“¥ Test Result Plot (New)",
                        data=f.read(),
                        file_name=f"{task_name_test}_test_result_plot.html",
                        mime="text/html")
                    
            results_pkl = st.session_state.latest_test_result
            if results_pkl and os.path.exists(results_pkl):
                with open(results_pkl, "rb") as f:
                    result_data = pickle.load(f)
                prediction_raw = result_data["prediction"]
                anomaly_raw = result_data["anomaly_score"]
                threshold_raw = result_data['threshold']

                df_pred = convert_to_df(prediction_raw, "prediction")
                df_anom = convert_to_df(anomaly_raw, "anomaly_score")
                df_result = df_pred.merge(df_anom, on="original_index").reset_index(drop=True)
                df_result['threshold'] = threshold_raw
                st.write("###### ğŸ“Š Prediction / Anomaly Score Table for Test Data")
                filtered_df = df_test_preview[df_test_preview["original_index"].isin(df_result["original_index"])].reset_index(drop=True).copy()
                for col in ["prediction", "anomaly_score", "threshold"]:
                    if col in filtered_df.columns:
                        filtered_df.drop(columns=[col], inplace=True)
                df_test_predicted = filtered_df.merge(df_result, on="original_index", how="left").reset_index(drop=True)

                normal_count = df_test_predicted['prediction'].value_counts().get(1, 0)
                fault_count = df_test_predicted['prediction'].value_counts().get(0, 0)
                total_count = normal_count + fault_count
                fault_ratio = fault_count / total_count if total_count > 0 else 0
                st.write("Normal Predict Count:", normal_count)
                st.write("Fault Predict Count:", fault_count)
                st.write("Predicted Fault Ratio (%):", round(fault_ratio * 100, 2), "%")
                st.write(df_test_predicted)
                st.session_state.df_test_preview = df_test_predicted.copy()

                csv = df_test_predicted.to_csv(index=False).encode("utf-8")
                st.download_button(label="ğŸ“¥ Download Predicted Test Data",data=csv,file_name=f"{task_name_test}_predicted.csv",mime="text/csv")
                
    st.info("ì¶”ë¡  ì™„ë£Œ ì´í›„ ì¸ê³¼ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    if st.session_state.df_test_preview is not None and st.session_state.get("test_run_done") and st.session_state.get("latest_test_result"):
        df_test_predicted_cut = st.session_state.df_test_preview
        with st.expander("###### ğŸ” Anomaly Detection Data êµ¬ê°„ ì„ íƒ ( ì¶”ë¡  ê²°ê³¼ ìƒì„¸ ê´€ì°°)", expanded=True):
            min_idx = int(df_test_predicted_cut["original_index"].min())
            max_idx = int(df_test_predicted_cut["original_index"].max())
            data_key = f"{min_idx}_{max_idx}"
            if "start_idx" not in st.session_state:
                st.session_state.start_idx = min_idx
            if "end_idx" not in st.session_state:
                st.session_state.end_idx = max_idx
            if "idx_range_key" not in st.session_state or st.session_state.idx_range_key != data_key:
                st.session_state.idx_range_key = data_key
                st.session_state.start_idx = min_idx
                st.session_state.end_idx = max_idx

            def sync_idx_range():
                st.session_state.start_idx = st.session_state.slider_range[0]
                st.session_state.end_idx = st.session_state.slider_range[1]
            st.slider("Original Index Range",min_value=min_idx,max_value=max_idx,value=(st.session_state.start_idx, st.session_state.end_idx),step=1,key="slider_range",on_change=sync_idx_range)
            start_idx = st.session_state.start_idx
            end_idx = st.session_state.end_idx
            col1, col2 = st.columns(2)
            with col1:
                st.number_input("Start Original Index",min_value=min_idx,max_value=max_idx,step=1,key="start_idx")
            with col2:
                st.number_input("End Original Index",min_value=min_idx,max_value=max_idx,step=1,key="end_idx")

            # ------------------- Validation -------------------
            if st.session_state.start_idx > st.session_state.end_idx:
                st.error("Start index must be less than or equal to End index.")
                st.session_state.start_idx = st.session_state.end_idx
            else:
                selected_df_new = df_test_predicted_cut[
                    (df_test_predicted_cut["original_index"] >= start_idx) &
                    (df_test_predicted_cut["original_index"] <= end_idx)].set_index("original_index") 
                selected_df_new ['original_index'] = selected_df_new.index         
                if "selected_df" not in st.session_state or st.session_state.selected_df is None or not selected_df_new.equals(st.session_state.selected_df):
                    st.session_state.selected_df = selected_df_new
                st.write(f"ğŸ“„ Rows Selected: {len(st.session_state.selected_df)}")
                st.dataframe(st.session_state.selected_df)
                csv = st.session_state.selected_df.to_csv(index=False).encode("utf-8")
                st.download_button(label="ğŸ“¥ Download Selected Data",data=csv,
                    file_name=f"{task_name_test}_test_selected_{start_idx}_{end_idx}.csv",mime="text/csv")
                
        selected_df = st.session_state.get("selected_df", None)
        toggle_test_selected_key = "df_test_selected_preview_toggle"
        default_toggle_selected_df = st.session_state.get(toggle_test_selected_key, False)
        show_summary_selected_df = st.checkbox("ğŸ“Š Anomaly Detection Data ìš”ì•½ í†µê³„ëŸ‰ ë³´ê¸°", value=default_toggle_selected_df, key=toggle_test_selected_key)
        if show_summary_selected_df:
            selected_df_summary = selected_df.drop(['file_number','original_index','segment_original_start','segment_original_end','prediction','anomaly_score','threshold'],axis=1).describe()
            if selected_df_summary.iloc[0:1].T.sum().values[0] !=0.0:
                st.dataframe(selected_df_summary, width=1200)  
                st.success("âœ… ìš”ì•½ í†µê³„ëŸ‰ í™•ì¸ ì™„ë£Œ")
            else:
                st.warning("âš  í˜„ì¬ êµ¬ê°„ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")


        toggle_plot_key_selected = "df_test_selected_plot_toggle"
        default_toggle_plot_selected = st.session_state.get(toggle_plot_key_selected, False)
        show_plot_selected = st.checkbox("ğŸ“ˆ Anomaly Detection Data ì‹œê°í™” ë³´ê¸°", value=default_toggle_plot_selected, key=toggle_plot_key_selected)
        if show_plot_selected:
            numeric_cols = selected_df.drop(['file_number','original_index','segment_original_start','segment_original_end','prediction','anomaly_score','threshold'],axis=1).select_dtypes(include="number").columns.tolist()
            try:
                min_idx = int(selected_df.index.min())
                max_idx = int(selected_df.index.max())
                multi_options = ["ğŸ”„ ì „ì²´ ë³€ìˆ˜ ì„ íƒ"] + numeric_cols
                selected_cols = st.multiselect("ğŸ“ˆ ì‹œê°í™”í•  ë³€ìˆ˜ë“¤ ì„ íƒ",multi_options,key="selected_multi_cols_selected") 
                if "ğŸ”„ ì „ì²´ ë³€ìˆ˜ ì„ íƒ" in selected_cols:
                    selected_cols = numeric_cols
                if selected_cols:
                    with st.expander("ğŸ“Š Plot ê²°ê³¼ ë³´ê¸°", expanded=True):
                        for col in selected_cols:
                            sns.set(font_scale=1.0)
                            fig, ax = plt.subplots(figsize=(12, 5))
                            ax.plot(selected_df[col].loc[start_idx:end_idx],color='blue')
                            ax.set_title(f"{col} Plot (Original Index {start_idx} ~ {end_idx})")
                            ax.set_xlabel("Index")
                            ax.set_ylabel(col)
                            st.pyplot(fig)
                            buf = io.BytesIO()
                            fig.savefig(buf, format="png")
                            buf.seek(0)
                            st.download_button(label=f"ğŸ“¥ {col} Plot Download",data=buf,file_name=f"{col}_Anomaly_Detection_plot.png",mime="image/png")
            except:
                st.warning("âš  í˜„ì¬ êµ¬ê°„ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    if st.session_state.df_test_preview is not None and st.session_state.get("test_run_done") and st.session_state.get("latest_test_result"):
        st.markdown("---")
        st.markdown(f"#### ğŸ”— Causal Analysis")
        with st.expander("##### â–¶ï¸ Phase 1", expanded=True):
            if "pcmci_analysis_logs" not in st.session_state:
                st.session_state.pcmci_analysis_logs = ""
            if "pcmci_analysis_run_done" not in st.session_state:
                st.session_state.pcmci_analysis_run_done = False

            # ---------------- í•„ìˆ˜ ì •ë³´ ì—…ë¡œë“œ ì¬í™•ì¸ & ì„¸ì…˜ ì—…ë°ì´íŠ¸ ----------------
            #### ê¸°ë³¸ ì„¤ì • ì •ë³´ 
            task_name_test = st.session_state.get("test_task_fault")
            feature_type = st.session_state.get("feature_type")
            gvw = st.session_state.get("gvw")
            drive_pattern = st.session_state.get("drive_pattern")

            #### ëª¨ë¸ ì„¤ì • ì •ë³´
            window_size = st.session_state["window_size"]
            batch_size = st.session_state["batch_size"]
            epoch = st.session_state["epoch"]

            #### Train / Test ê²°ê³¼ ê²½ë¡œ ì„¤ì • ì •ë³´
            if "TB" in task_name_test or "turbo" in task_name_test:
                main_task = "Turbo_problem"
            elif "egr_sw" in task_name_test:
                main_task = "EGR_SW"
            elif "egr_hw" in task_name_test:
                main_task = "EGR_HW"
            else:
                main_task = task_name_test
            result_path = os.path.join(BASE_DIR, "result", main_task,f"{main_task}_{feature_type}_gvw_{gvw}_{drive_pattern}_{task_name_test}_(Train)")
            ckpt_load = find_latest_train_checkpoint(result_path, task_name_test)
            st.session_state["latest_train_ckpt_dir"] = ckpt_load
            ckpt_dir = st.session_state["latest_train_ckpt_dir"]
            result_train_path = os.path.dirname(ckpt_dir)
            use_calibration = st.session_state.use_calibration
            if use_calibration == True:
                result_path_test = os.path.join(BASE_DIR, "result", main_task,f"Target_Inference_{main_task}_{feature_type}_gvw_{gvw}_{drive_pattern}_{task_name_test}")
            else:
                result_path_test = os.path.join(BASE_DIR, "result", main_task,f"Target_Inference_No_Calibration_{main_task}_{feature_type}_gvw_{gvw}_{drive_pattern}_{task_name_test}")
            latest_result = find_latest_test_result(result_path_test, task_name_test)
            st.session_state["latest_test_result"] = latest_result
            result_dir = st.session_state["latest_test_result"] 
            result_test_path = os.path.dirname(result_dir)

            #### Train / Test ë°ì´í„° ê²½ë¡œ ì„¤ì • ì •ë³´ 
            normal_dataset = st.session_state.get("train_csv_path")
            abnormal_dataset = st.session_state.get("test_csv_path")
            column_list_path = st.session_state.get("column_list")

            
            #### Log & Save Directory ìƒì„±
            save_dir = os.path.join(BASE_DIR, "causal_trace","results",f"{main_task}_{feature_type}_gvw_{gvw}_{drive_pattern}_{task_name_test}")
            os.makedirs(save_dir, exist_ok=True)
            log_dir = os.path.join(BASE_DIR,"causal_trace", "logs",f"{task_name_test}")
            os.makedirs(log_dir, exist_ok=True)
            log_file = os.path.join(log_dir,f"{main_task}_{feature_type}_gvw_{gvw}_{drive_pattern}_{task_name_test}.txt")

            # ------------------- CFG_DICT & Params êµ¬ì„± -------------------
            cfg_dict = {
                "log_dir": log_dir + "/",    # ìœ„ì—ì„œ ì‘ì„±í•œ log_dir í™œìš©
                "save_dir_compare_result": save_dir + "/",
                "task": f"{main_task}_{feature_type}_gvw_{gvw}_{drive_pattern}_{task_name_test}",
                "vars_highlight": "None",

                "abnormal": {
                    "dataset_path": abnormal_dataset,
                    "save_dir": os.path.join(save_dir, "abnormal"),
                    "problem_type": main_task,
                    "gvw": gvw,
                    "drive_pattern": drive_pattern,
                    "feature_type": feature_type},

                "normal": {
                    "dataset_path": normal_dataset,
                    "save_dir": os.path.join(save_dir, "normal"),
                    "problem_type": main_task,
                    "gvw": gvw,
                    "drive_pattern": drive_pattern,
                    "feature_type": feature_type},

                "result_train_path": result_train_path,
                "result_test_path": result_test_path}
            
            base_params_pcmci = {
                "downsample_rate": 1,
                "cond_ind_test": "PARCORR",
                "tau_max": 5,
                "tau_min": 0,
                "min_length": 200,
                "max_length": 1000,
                "alpha_level": 0.05,
                "combine_segments": True,
                "remove_bidirectional": True,
                "use_parallel": True}

            cfg_json_path = os.path.join(log_dir, "cfg_dict_input.json")
            with open(cfg_json_path, "w") as f:
                json.dump(cfg_dict,f)
            pcmci_json_path = os.path.join(log_dir, "base_params_pcmci_input.json")
            with open(pcmci_json_path, "w") as f:
                json.dump(base_params_pcmci,f)

            defaults = {
                "pcmci_log_queue": queue.Queue(),
                "pcmci_logs_buffer": "",
                "pcmci_analysis_logs": "",
                "pcmci_analysis_run_done": False,
                "pcmci_running": False,
                "pcmci_process": None,
                "pcmci_progress": 0,
                "pcmci_status": "ëŒ€ê¸° ì¤‘"}
            for k, v in defaults.items():
                if k not in st.session_state:
                    st.session_state[k] = v

            # ------------------- ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ í•¨ìˆ˜ -------------------
            def run_pcmci_bg(cmd, env, log_file, log_queue):
                try:
                    log_queue.put(("STATUS", "PCMCI í”„ë¡œì„¸ìŠ¤ ì‹œì‘"))
                    log_queue.put(("PROGRESS", 10))
                    process = subprocess.Popen(cmd,stdout=subprocess.PIPE,stderr=subprocess.STDOUT,text=True,env=env)
                    log_queue.put(("PROCESS", process))
                    log_queue.put(("STATUS", "PCMCI ì‹¤í–‰ ì¤‘"))
                    log_queue.put(("PROGRESS", 30))
                    for line in iter(process.stdout.readline, ""):
                        log_queue.put(("LOG", line))
                        if "PCMCI finished" in line:
                            log_queue.put(("PROGRESS", 90))
                    process.stdout.close()
                    rc = process.wait()
                    log_queue.put(("PROGRESS", 100))
                    log_queue.put(("DONE", rc == 0))
                    with open(log_file, "w", encoding="utf-8") as f:
                        f.write("".join([]))  
                except Exception as e:
                    log_queue.put(("ERROR", str(e)))

            col1, col2 = st.columns(2)
            with col1:
                if st.button("ğŸš€ PCMCI ë¶„ì„", disabled=st.session_state.pcmci_running):
                    st.session_state.pcmci_running = True
                    st.session_state.pcmci_analysis_run_done = False
                    st.session_state.pcmci_logs_buffer = ""
                    st.session_state.pcmci_progress = 0
                    st.session_state.pcmci_status = "ì´ˆê¸°í™”"
                    cmd = [
                        sys.executable,
                        os.path.join(BASE_DIR, "causal_trace", "causal_trace.py"),
                        "--cfg_dict", cfg_json_path,
                        "--base_params_pcmci", pcmci_json_path,
                        "--dir_log", log_dir,
                        "--seed", "42"]
                    env = os.environ.copy()
                    env["CUDA_VISIBLE_DEVICES"] = "0"
                    threading.Thread(target=run_pcmci_bg,args=(cmd, env, log_file, st.session_state.pcmci_log_queue),daemon=True,).start()
            with col2:
                if st.button("â›” ê°•ì œ ì¤‘ë‹¨", disabled=not st.session_state.pcmci_running):
                    proc = st.session_state.pcmci_process
                    if proc:
                        proc.kill()
                        st.session_state.pcmci_status = "ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨"
                        st.session_state.pcmci_running = False
                        st.session_state.pcmci_analysis_run_done = True
                        st.warning("PCMCI í”„ë¡œì„¸ìŠ¤ê°€ ê°•ì œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

            try:
                while True:
                    msg = st.session_state.pcmci_log_queue.get_nowait()
                    msg_type, payload = msg
                    if msg_type == "LOG":
                        st.session_state.pcmci_logs_buffer += payload
                    elif msg_type == "STATUS":
                        st.session_state.pcmci_status = payload
                    elif msg_type == "PROGRESS":
                        st.session_state.pcmci_progress = payload
                    elif msg_type == "PROCESS":
                        st.session_state.pcmci_process = payload
                    elif msg_type == "DONE":
                        st.session_state.pcmci_analysis_run_done = payload
                        st.session_state.pcmci_running = False
                        st.session_state.pcmci_analysis_logs = st.session_state.pcmci_logs_buffer
                    elif msg_type == "ERROR":
                        st.session_state.pcmci_status = "ì˜¤ë¥˜ ë°œìƒ"
                        st.error(payload)
            except queue.Empty:
                pass
            
            col1, col2 = st.columns([15, 1])  # ë¹„ìœ¨ë¡œ ë„ˆë¹„ ì¡°ì • ê°€ëŠ¥
            with col1:
                st.markdown(f"**ìƒíƒœ:** {st.session_state.pcmci_status} (Background Run)")
            with col2:
                st.button("?", help="ë‹¤ë¥¸ ê¸°ëŠ¥ì— ë°©í•´ë°›ì§€ ì•Šê³  ìˆ˜í–‰ë©ë‹ˆë‹¤.\n\nLogì— ì¦‰ê°ì ìœ¼ë¡œ ë¶„ì„ê³¼ì •ì´ ë‚˜íƒ€ë‚˜ì§€ ì•Šì„ ë•ŒëŠ” ë‹¤ë¥¸ í˜ì´ì§€ë¡œ ì ê¹ ì´ë™ í˜¹ì€ ê¸°ëŠ¥ ìˆ˜í–‰(Ex.ğŸ“ˆ Anomaly Detection Data ì‹œê°í™” ë³´ê¸°)ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
            st.progress(st.session_state.pcmci_progress)
            st.text_area("PCMCI ë¡œê·¸",value=st.session_state.pcmci_logs_buffer[-5000:],height=300,key="pcmci_log_area",)
            if st.session_state.pcmci_analysis_run_done and st.session_state.pcmci_analysis_logs:
                st.download_button("ğŸ“„ PCMCI ë¡œê·¸ ë‹¤ìš´ë¡œë“œ",data=st.session_state.pcmci_analysis_logs,file_name="pcmci_analysis.log",mime="text/plain",)
            if st.session_state.pcmci_running:
                time.sleep(0.5)
                #st.experimental_rerun()
            elif st.session_state.pcmci_analysis_run_done:
                st.success("âœ… PCMCI ë¶„ì„ ì¢…ë£Œ")


            # ------------------- Phase 2 í™œì„±í™” -------------------
            pcmci_normal_path = os.path.join(save_dir, "normal", "pcmci_results.pkl")
            pcmci_abnormal_path = os.path.join(save_dir, "abnormal", "pcmci_results.pkl")
            if st.session_state.pcmci_analysis_run_done:
                if os.path.exists(pcmci_normal_path) and os.path.exists(pcmci_abnormal_path):
                    st.success("âœ… Normal / Abnormal PCMCI ë¶„ì„ ê²°ê³¼ ì™„ë£Œ. Phase 2ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                    with open(column_list_path, "r", encoding="utf-8") as f:
                        columns = [line.strip() for line in f.readlines() if line.strip()]
                else:
                    st.warning("âš ï¸ í˜„ì¬ PCMCI ë¶„ì„ì— ë¶€ì í•©í•œ Normal or Abnormal ë°ì´í„°ì…‹ì´ ì¡´ì¬í•©ë‹ˆë‹¤. ë¶„ì„ ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

           
            # =================== Phase 2 State ===================
            defaults_phase2 = {
                "causal_log_queue": queue.Queue(),
                "causal_logs_buffer": "",
                "causal_analysis_logs": "",
                "causal_running": False,
                "causal_run_done": False,
                "causal_process": None,
                "causal_progress": 0,
                "causal_status": "ëŒ€ê¸° ì¤‘",
                "feature_importance_type": "ttest",
                "target_variable": None}
            for k, v in defaults_phase2.items():
                if k not in st.session_state:
                    st.session_state[k] = v

            def run_causal_bg(cmd, env, log_queue):
                try:
                    log_queue.put(("STATUS", "Causal Analysis ì‹œì‘"))
                    log_queue.put(("PROGRESS", 10))
                    process = subprocess.Popen(
                        cmd,
                        stdout=subprocess.PIPE,
                        stderr=subprocess.STDOUT,
                        text=True,
                        env=env)
                    log_queue.put(("PROCESS", process))
                    log_queue.put(("STATUS", "Causal Analysis ì‹¤í–‰ ì¤‘"))
                    log_queue.put(("PROGRESS", 30))
                    for line in iter(process.stdout.readline, ""):
                        log_queue.put(("LOG", line))
                        if any(k in line.lower() for k in ["ttest", "cohen", "shap", "finished"]):
                            log_queue.put(("PROGRESS", 90))
                    process.stdout.close()
                    rc = process.wait()
                    log_queue.put(("PROGRESS", 100))
                    log_queue.put(("DONE", rc == 0))
                except Exception as e:
                    log_queue.put(("ERROR", str(e)))


        with st.expander("##### â–¶ï¸ Phase 2", expanded=True):
            # Default State
            if "threshold" not in st.session_state:
                st.session_state.threshold = 0.2
            if "max_depth" not in st.session_state:
                st.session_state.max_depth = 3
            if "top_k" not in st.session_state:
                st.session_state.top_k = 3
            with st.sidebar.expander("ğŸ“Œ ì¸ê³¼ë¶„ì„ íŒŒë¼ë¯¸í„° ì„¤ì •", expanded=False):
                threshold = st.number_input("Threshold",min_value=0.0,max_value=1.0,value=st.session_state.threshold,step=0.01)
                max_depth = st.number_input("Max Depth",min_value=1,max_value=10,value=st.session_state.max_depth,step=1)
                top_k = st.number_input("Top K",min_value=1,max_value=20,value=st.session_state.top_k,step=1)

            base_params_causal_trace = {
                "window_size": window_size,
                "batch_size": batch_size,
                "threshold": threshold,
                "max_depth": max_depth,
                "top_k": top_k}
            
            col1, col2 = st.columns([15, 1])  # ë¹„ìœ¨ë¡œ ë„ˆë¹„ ì¡°ì • ê°€ëŠ¥
            with col1:
                st.write("Causal Analysis Parameters ì¡°ì •:", base_params_causal_trace)
            with col2:
                st.button("?", help="Phase2ë¥¼ ìˆ˜í–‰í•˜ê¸° ìœ„í•œ ì¸ê³¼ë¶„ì„ íŒŒë¼ë¯¸í„° ì§‘í•©ì…ë‹ˆë‹¤. \n\n Sidebar ë©”ë‰´ì˜ ğŸ“Œ ì¸ê³¼ë¶„ì„ íŒŒë¼ë¯¸í„° ì„¤ì •ì„ í†µí•´ì„œ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            causal_json_path = os.path.join(log_dir, "base_params_causal_trace.json")
            with open(causal_json_path, "w") as f:
                json.dump(base_params_causal_trace,f)

            col_run, col_stop = st.columns(2)
            # â–¶ ì‹¤í–‰
            with col_run:
                if st.button("ğŸš€ Causal Trace & Feature Importance ë¶„ì„",disabled=st.session_state.causal_running):
                    if not st.session_state.pcmci_analysis_run_done:
                        st.warning("âš ï¸ PCMCI Phase 1ì„ ë¨¼ì € ì™„ë£Œí•˜ì„¸ìš”.")
                    else:
                        st.session_state.causal_running = True
                        st.session_state.causal_run_done = False
                        st.session_state.causal_logs_buffer = ""
                        st.session_state.causal_progress = 0
                        st.session_state.causal_status = "ì´ˆê¸°í™”"
                        cmd = [
                            sys.executable,
                            os.path.join(BASE_DIR, "causal_trace", "causal_trace.py"),
                            "--cfg_dict", cfg_json_path,
                            "--base_params_pcmci", pcmci_json_path,
                            "--base_params_causal_trace", causal_json_path,
                            "--dir_log", log_dir,
                            "--seed", "42"]
                        env = os.environ.copy()
                        env["CUDA_VISIBLE_DEVICES"] = "0"
                        threading.Thread(target=run_causal_bg,args=(cmd, env, st.session_state.causal_log_queue),daemon=True,).start()

            # â›” ê°•ì œ ì¤‘ë‹¨
            with col_stop:
                if st.button("â›” ê°•ì œ ì¤‘ë‹¨.", disabled=not st.session_state.causal_running):
                    proc = st.session_state.causal_process
                    if proc:
                        proc.kill()
                        st.session_state.causal_status = "ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨"
                        st.session_state.causal_running = False
                        st.session_state.causal_run_done = True
                        st.warning("Causal Analysis í”„ë¡œì„¸ìŠ¤ê°€ ê°•ì œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
            try:
                while True:
                    msg_type, payload = st.session_state.causal_log_queue.get_nowait()
                    if msg_type == "LOG":
                        st.session_state.causal_logs_buffer += payload
                    elif msg_type == "STATUS":
                        st.session_state.causal_status = payload
                    elif msg_type == "PROGRESS":
                        st.session_state.causal_progress = payload
                    elif msg_type == "PROCESS":
                        st.session_state.causal_process = payload
                    elif msg_type == "DONE":
                        st.session_state.causal_run_done = payload
                        st.session_state.causal_running = False
                        st.session_state.causal_analysis_logs = st.session_state.causal_logs_buffer
                    elif msg_type == "ERROR":
                        st.session_state.causal_status = "ì˜¤ë¥˜ ë°œìƒ"
                        st.error(payload)
            except queue.Empty:
                pass


            col1, col2 = st.columns([15, 1])  # ë¹„ìœ¨ë¡œ ë„ˆë¹„ ì¡°ì • ê°€ëŠ¥
            with col1:
                st.markdown(f"**ìƒíƒœ:** {st.session_state.causal_status} (Background Run)")
            with col2:
                st.button("?", help="Phase1ê³¼ ë™ì¼í•˜ê²Œ ë‹¤ë¥¸ ê¸°ëŠ¥ì— ë°©í•´ë°›ì§€ ì•Šê³  ìˆ˜í–‰ë©ë‹ˆë‹¤. \n\nLogì— ì¦‰ê°ì ìœ¼ë¡œ ë¶„ì„ê³¼ì •ì´ ë‚˜íƒ€ë‚˜ì§€ ì•Šì„ ë•ŒëŠ” ë‹¤ë¥¸ í˜ì´ì§€ë¡œ ì ê¹ ì´ë™ í˜¹ì€ ê¸°ëŠ¥ ìˆ˜í–‰\n\n(Ex. Sidebar ë©”ë‰´ì˜ ğŸ¯ ì¸ê³¼ë¶„ì„ ëŒ€ìƒ ë³€ìˆ˜ ì„ íƒ)ì„ ê¶Œì¥í•©ë‹ˆë‹¤.")
            st.progress(st.session_state.causal_progress)
            st.text_area("Causal Trace & Feature Importance ë¶„ì„ ë¡œê·¸",value=st.session_state.causal_logs_buffer[-5000:],height=300,key="causal_log_area")
            completed_targets = []
            incomplete_targets = []
            with open(column_list_path, "r", encoding="utf-8") as f:
                columns = [line.strip() for line in f.readlines() if line.strip()]
            for col in columns:
                tree_png_all = os.path.join(save_dir, f"causal_tree_{col}.png")
                critical_csv_all = os.path.join(save_dir, f"critical_paths_result_{col}.csv")
                variable_contribution_all = os.path.join(save_dir, f"variable_contribution_{col}.csv")
                if os.path.exists(tree_png_all) and os.path.exists(critical_csv_all) and os.path.exists(variable_contribution_all):
                    completed_targets.append(col)
                else:
                    incomplete_targets.append(col)

            if completed_targets and not incomplete_targets:
                # âœ… ALL DONE
                st.success("âœ… ëª¨ë“  Targetì— ëŒ€í•œ Causal Trace ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n\n"
                    "Feature Importance ê³„ì‚° ì™„ë£Œ ì´ì „ê¹Œì§€ ì¸ê³¼ë¶„ì„ ê²°ê³¼ë¶€í„° í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            elif completed_targets and incomplete_targets:
                st.warning("âš ï¸ í˜„ì¬ ì¼ë¶€ ë³€ìˆ˜ì— ëŒ€í•œ Causal Trace ë¶„ì„ì´ ë¶ˆê°€í•©ë‹ˆë‹¤.\n\n"
                    f"ì™„ë£Œ: {len(completed_targets)} / ì „ì²´: {len(columns)}\n\n")
            else:
                st.info("â„¹ï¸ ì•„ì§ Causal Trace ê²°ê³¼ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n\n""ë¶„ì„ì´ ì§„í–‰ ì¤‘ì´ê±°ë‚˜ ëŒ€ê¸° ìƒíƒœì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")


            if completed_targets:
                columns = completed_targets.copy()
            else:
                columns = columns
            if "target_variable" not in st.session_state:
                st.session_state.target_variable = columns[0]
            with st.sidebar.expander("ğŸ¯ ì¸ê³¼ë¶„ì„ ëŒ€ìƒ ë³€ìˆ˜ ì„ íƒ", expanded=False):
                target_variable = st.selectbox("Target Variable",columns,index=columns.index(st.session_state.target_variable)if st.session_state.target_variable in columns else 0)
                

            tree_png = os.path.join(save_dir, f"causal_tree_{target_variable}.png")
            critical_csv = os.path.join(save_dir, f"critical_paths_result_{target_variable}.csv")
            variable_contribution_csv = os.path.join(save_dir, f"variable_contribution_{target_variable}.csv")

            
            # ---------- Feature Importance Typeë³„ ê²°ê³¼ í™•ì¸ ----------
            importance_types = ["ttest", "cohend", "shap"]
            existing_importance_types = [t for t in importance_types if os.path.exists(os.path.join(save_dir, f"feature_importance_{t}.csv"))]
            if existing_importance_types:
                if ("feature_importance_type" not in st.session_state or st.session_state.feature_importance_type not in existing_importance_types):
                    st.session_state.feature_importance_type = existing_importance_types[0]
                with st.sidebar.expander("âš™ï¸ Feature Importance ì„¤ì •", expanded=False):
                    feature_importance_type = st.selectbox("ğŸ“Œ Feature Importance Type",existing_importance_types,index=existing_importance_types.index(st.session_state.feature_importance_type))
                
                importance_csv = os.path.join(save_dir,f"feature_importance_{feature_importance_type}.csv")
                top_features = pd.read_csv(importance_csv)['Feature'].head(5).tolist()
                top_features_str = ", ".join(top_features) if top_features else "N/A"
                feature_type_map = {"ttest": "T-Test Analysis","cohend": "Cohen's D-Test Analysis","shap": "TimeSHAP Analysis"}
                st.table(pd.DataFrame({
                    "Select Download Types": ["Feature Importance Type","Top 5 Features"],
                    "Value": [feature_type_map.get(feature_importance_type, "Unknown"),top_features_str]}))

                col1, col2 = st.columns([15, 1])  # ë¹„ìœ¨ë¡œ ë„ˆë¹„ ì¡°ì • ê°€ëŠ¥
                with col1:
                    st.info(f"ğŸ¯ Causal Analysis Target: {target_variable}")
                with col2:
                    st.button("?", help="ì´ìƒíƒì§€ì— ëŒ€í•œ ì›ì¸ë³€ìˆ˜ ì¤‘ìš”ë„ ë° íŠ¹ì • ë³€ìˆ˜ì— ëŒ€í•œ ì¸ê³¼ë¶„ì„ ê²°ê³¼ë¥¼ ê´€ì°°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n\n Sidebar ë©”ë‰´ì˜ âš™ï¸ Feature Importance ì„¤ì •ì„ í†µí•´ì„œ ìƒìœ„5ê°œì˜ ì£¼ìš”ë³€ìˆ˜ë“¤ì„ ìš°ì„  ì„ ë³„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            

                col1, col2, col3 , col4 = st.columns(4)
                with col1:
                    if os.path.exists(tree_png):
                        with open(tree_png, "rb") as f:
                            st.download_button(f"ğŸ“¥ Causal Tree",f,os.path.basename(tree_png),"image/png",use_container_width=True)
                    else:
                        st.info("Causal Tree ê²°ê³¼ ì—†ìŒ")
                with col2:
                    if os.path.exists(critical_csv):
                        with open(critical_csv, "rb") as f:
                            st.download_button("ğŸ“¥ Critical Paths",f,os.path.basename(critical_csv),"text/csv",use_container_width=True)
                    else:
                        st.info("Critical Paths ê²°ê³¼ ì—†ìŒ")
                with col3:
                    if os.path.exists(variable_contribution_csv):
                        with open(variable_contribution_csv, "rb") as f:
                            st.download_button("ğŸ“¥ Variable Contributions",f,os.path.basename(variable_contribution_csv),"text/csv",use_container_width=True)
                    else:
                        st.info("Variable Contributions ê²°ê³¼ ì—†ìŒ")

                with col4:
                    if os.path.exists(importance_csv):
                        with open(importance_csv, "rb") as f:
                            st.download_button(f"ğŸ“¥ Feature Importance",f,os.path.basename(importance_csv),"text/csv",use_container_width=True)
                    else:
                        st.info("Feature Importance ê²°ê³¼ ì—†ìŒ")
            else:
                col1, col2 = st.columns(2)
                with col1:
                    if os.path.exists(tree_png):
                        with open(tree_png, "rb") as f:
                            st.download_button("ğŸ“¥ Causal Tree",f,os.path.basename(tree_png),"image/png",use_container_width=True)
                    else:
                        st.info("Causal Tree ê²°ê³¼ ì—†ìŒ")
                with col2:
                    if os.path.exists(critical_csv):
                        with open(critical_csv, "rb") as f:
                            st.download_button("ğŸ“¥ Critical Paths",f,os.path.basename(critical_csv),"text/csv",use_container_width=True)
                    else:
                        st.info("Critical Paths ê²°ê³¼ ì—†ìŒ")


            # ---------- 7. ì‹¤í–‰ ìƒíƒœ UX ----------
            if st.session_state.causal_running:
                st.warning("â³ ë¶„ì„ì´ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. ìƒì„±ëœ ê²°ê³¼ë¶€í„° í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            elif st.session_state.causal_run_done:
                st.success("âœ… Causal Analysis ì™„ë£Œ")


    
######################################################################################################################################################
#######################################################################################################################################################
#######################################################################################################################################################
#######################################################################################################################################################
#######################################################################################################################################################
#######################################################################################################################################################


if page== "Trained Model Fine-Tuning":
    top_left, top_right = st.columns([4, 1])
    with top_right:
        #pdf_bytes = generate_summary_pdf(get_page_summary(3))
        summary_text = load_summary_txt(page=3, base_dir=BASE_DIR)
        pdf_bytes = generate_summary_pdf(summary_text)
        st.download_button(label="ğŸ“„ê¸°ëŠ¥ ì„¤ëª…",data=pdf_bytes,file_name="ê¸°ëŠ¥ì„¤ëª…_Page3.pdf",mime="application/pdf",use_container_width=True)
    st.markdown(f"#### ğŸ› ï¸ Trained Model Fine-Tuning")

    df_train_preview = st.session_state.get("df_train_preview")
    task_name_train = st.session_state.get("task_name_train")
    task_name_test = st.session_state.get("test_task_fault")
    feature_type = st.session_state.get("feature_type") or st.session_state.common_feature_type
    gvw = st.session_state.get("gvw") or st.session_state.common_gvw
    drive_pattern = st.session_state.get("drive_pattern")
    common_min_length = st.session_state.get("common_min_length") or st.session_state.common_min_length
    engine_threshold = st.session_state.get("engine_threshold") or st.session_state.engine_threshold
    engine_threshold_high = st.session_state.get("engine_threshold_high") or st.session_state.engine_threshold_high
    fuel_threshold = st.session_state.get("fuel_threshold")
    module = st.session_state.get("preprocessing_module")
    log_path = os.path.join(BASE_DIR, "logs")
    os.makedirs(log_path, exist_ok=True)

    if "TB" in task_name_test or "turbo" in task_name_test:
        main_task = "Turbo_problem"
    elif "egr_sw" in task_name_test:
        main_task = "EGR_SW"
    elif "egr_hw" in task_name_test:
        main_task = "EGR_HW"
    else:
        main_task = task_name_test
    result_path = os.path.join(BASE_DIR, "result", main_task,f"{main_task}_{feature_type}_gvw_{gvw}_{drive_pattern}_{task_name_test}_(Train)")
    ckpt_recent = find_latest_train_checkpoint(result_path, task_name_test)
    if ckpt_recent ==None:
        st.warning("âš ï¸ í•™ìŠµ ë° ì¶”ë¡ ì„ ìˆ˜í–‰í•  ì¤€ë¹„ê°€ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ .\n\n"
                   "ë¨¼ì € **Data Upload and Preprocess** í˜ì´ì§€ì˜ ì‘ì—…ì„ ì™„ì„±í•˜ì„¸ìš”.")
        st.stop()  

    required_keys = ['test_task_fault','window_size','feature_type','gvw','drive_pattern']#['feature_type'] 
    missing = [k for k in required_keys if k not in st.session_state or st.session_state[k] in [None, ""]]
    if missing:
        st.warning("âš ï¸ í•™ìŠµ ë° ì¶”ë¡ ì„ ìˆ˜í–‰í•  ì¤€ë¹„ê°€ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤ .\n\n"
                   "ë¨¼ì € **Data Upload and Preprocess** í˜ì´ì§€ì˜ ì‘ì—…ì„ ì™„ì„±í•˜ì„¸ìš”.")
        st.warning("âš ï¸ ê¸°ì¡´ í•™ìŠµì´ë ¥ì´ ì¡´ì¬í•  ê²½ìš° ğŸ“‚ Train ë°ì´í„° ì—…ë¡œë“œ & ğŸ“Œ ëª¨ë¸ íŒŒë¼ë¯¸í„° ì„¤ì • ë²„íŠ¼ì„ ëˆ„ë¥´ì„¸ìš”.")
        st.stop()  


    with st.expander("##### ğŸ“Œ TranAD Fine-Tuning", expanded=True):

        if "fine_tune_save_dir" not in st.session_state:
            st.session_state.fine_tune_save_dir = None
        col1, col2 = st.columns([15, 1])  # ë¹„ìœ¨ë¡œ ë„ˆë¹„ ì¡°ì • ê°€ëŠ¥
        with col1:
            df_fine_tune_normal, fine_tune_save_dir_normal = data_upload_ui(st,"Fine_Tune","fine_tuning_normal",mode="fine_tune",task_name="Normal_Fine_Tuning",module=module)
        with col2:
            st.button("?", help="í˜„ì¬ ì—”ì§„ì— ëŒ€í•œ ì¶”ê°€ ì •ìƒë°ì´í„°ì— ëŒ€í•œ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. \n\nì „ì²˜ë¦¬ëŠ” 1í˜ì´ì§€ì˜ âš™ï¸ê³µí†µ ì„¤ì •/ğŸ”§ì£¼í–‰íŒ¨í„´ ì„¸ë¶€ì‚¬í•­ ì„¤ì •ê°’ì„ í† ëŒ€ë¡œ ì§„í–‰ë©ë‹ˆë‹¤.")

        if df_fine_tune_normal is not None:
            st.session_state["df_fine_tune_preview"] = df_fine_tune_normal
        if fine_tune_save_dir_normal is not None:
            st.session_state["fine_tune_save_dir"] = fine_tune_save_dir_normal
        df_fine_tune_preview = st.session_state.get("df_fine_tune_preview")
        fine_tune_path_dir = st.session_state["fine_tune_save_dir"]

        if "train_fine_tuning_logs" not in st.session_state:
            st.session_state.train_fine_tuning_logs = ""
        if "train_fine_tuning_run_done" not in st.session_state:
            st.session_state.train_fine_tuning_run_done = False
        result_path = os.path.join(BASE_DIR, "result", main_task,f"{main_task}_{feature_type}_gvw_{gvw}_{drive_pattern}_{task_name_test}_(Train)")
        ckpt_load = find_latest_train_checkpoint(result_path, task_name_test)
        st.session_state["latest_train_ckpt_dir"] = ckpt_load
        log_path = os.path.join(BASE_DIR, "logs")
        log_file = os.path.join(log_path,f"{main_task}_{feature_type}_gvw_{gvw}_{drive_pattern}_{task_name_test}_finetuned.txt")

        window_size = st.session_state["window_size"]
        batch_size = st.session_state["batch_size"]
        epoch = st.session_state["epoch"]
        learning_rate = st.session_state["learning_rate"]
        try:
            train_path_dir = st.session_state["train_path_dir"]
            eval_path_dir = st.session_state["eval_path_dir"]
            test_path_dir = st.session_state["test_save_dir"]
        except:
            train_path_dir = os.path.join(BASE_DIR,"data_preprocessed", f"normal_gvw{gvw}", feature_type, drive_pattern)
            eval_path_dir = os.path.join(BASE_DIR,"data_preprocessed",f"Eval_{task_name_test}",feature_type,drive_pattern)
            test_path_dir =  os.path.join(BASE_DIR,"data_preprocessed",task_name_test,feature_type,drive_pattern)
        ckpt_dir = st.session_state["latest_train_ckpt_dir"]
        column_list_path = st.session_state.get("column_list")

        if st.button("â–¶ï¸ ì •ìƒë°ì´í„° ì¶”ê°€í•™ìŠµ ì‹¤í–‰"): 
            col1, col2 = st.columns([15, 1])  # ë¹„ìœ¨ë¡œ ë„ˆë¹„ ì¡°ì • ê°€ëŠ¥
            with col1:
                st.info('í˜„ì¬ Taskì— ëŒ€í•œ Train / Eval ë°ì´í„°ë¥¼ í™œìš©í•œ ëª¨ë¸ ê°€ì¤‘ì¹˜ ì—…ë°ì´íŠ¸ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤.')
            with col2:
                st.button("?", help="1í˜ì´ì§€ì—ì„œ ì„ íƒí•œ í•™ìŠµëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. í•´ë‹¹ ê³¼ì •ì€ ì¶”í›„ ë¶ˆëŸ‰ ì˜¤íƒì§€ì— ëŒ€í•œ ë°œìƒê°€ëŠ¥ì„±ì„ ì™„í™”í•©ë‹ˆë‹¤.")

            cmd = (
                f"python main.py "
                f"--model TranAD "
                f"--task_name \"{task_name_test}\" "
                f"--train_dataset \"{os.path.join(fine_tune_path_dir, 'data.csv')}\" "
                f"--eval_dataset \"{os.path.join(eval_path_dir, 'data.csv')}\" "
                f"--columns \"{column_list_path}\" "
                f"--epoch {epoch} "
                f"--window_size {window_size} "
                f"--batch_size {batch_size} "
                f"--save_result_dir \"{result_path}\" "
                f"--phase train "
                f"--finetune "
                f"--model_path \"{ckpt_dir}\"")
            
            env = os.environ.copy()
            env["CUDA_VISIBLE_DEVICES"] = "0"
            with st.spinner("í•™ìŠµ ì§„í–‰ ì¤‘..."):
                rc, logs = run_subprocess_and_stream(cmd, env=env)
            st.session_state.train_fine_tuning_logs = logs
            st.session_state.train_fine_tuning_run_done = (rc == 0)
            try:
                with open(log_file, "w", encoding="utf-8") as f:
                    f.write(logs)
                st.success(f"í•™ìŠµ ë¡œê·¸ ì €ì¥ë¨: {log_file}")
            except:
                st.error("ë¡œê·¸ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨")
            if rc == 0:
                ckpt = find_latest_train_checkpoint(result_path, task_name_test)
                if ckpt:
                    ckpt_dir = os.path.dirname(ckpt)
                    st.session_state.latest_train_ckpt_dir = ckpt
                    st.success(f"í•™ìŠµ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì™„ë£Œ: {ckpt}")
                else:
                    st.error("ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. main.py ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
            else:
                st.error("í•™ìŠµ ì‹¤íŒ¨. ë¡œê·¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.")

        # ---------------- Training Fine-Tuning Graph & Anomaly Score Plot ë‹¤ìš´ë¡œë“œ ----------------
        if st.session_state.train_fine_tuning_run_done and st.session_state.get("latest_train_ckpt_dir"):
            st.text_area("Fine-Tuning ë¡œê·¸", value=st.session_state.train_fine_tuning_logs[-5000:], height=300)
            col1, col2 = st.columns(2)  
            pdf_path = os.path.join(os.path.dirname(st.session_state.latest_train_ckpt_dir),"train_loss_plot.pdf")
            with col1:
                if os.path.exists(pdf_path):
                    with open(pdf_path, "rb") as f:
                        st.download_button(label="ğŸ“¥ Train Loss Plot (Fine-Tuned)",data=f.read(),
                            file_name="train_loss_plot.pdf",mime="application/pdf",use_container_width=True)

            html_plot_path = os.path.join(os.path.dirname(st.session_state.latest_train_ckpt_dir),"anomaly_score.html")
            with col2:
                if os.path.exists(html_plot_path):
                    with open(html_plot_path, "rb") as f:
                        st.download_button(label="ğŸ“¥ Train Result Plot (Fine-Tuned)",data=f.read(),
                            file_name="train_result_plot.html",mime="text/html",use_container_width=True)
                        
        try:
            data_params = {
                "Data Shape": df_train_preview.shape,
                "Train Task": task_name_train,
                "Test Task": task_name_test,
                "Feature Type": feature_type,
                "GVW": gvw,
                "Drive Pattern": drive_pattern,
                "Min Length": common_min_length,
                "Engine Threshold": engine_threshold,
                "Engine Threshold High": engine_threshold_high,
                "Fuel Threshold": fuel_threshold}
        except Exception as e:
            st.error(f"âš  data_params ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            data_params = {}
                        
        active_list = [data_params]
        try:
            save_path = os.path.join(BASE_DIR,result_path,task_name_test, "data_preprocessing_parameters.json")
            with open(save_path, "w", encoding="utf-8") as f:
                json.dump(active_list, f, indent=4)
            train_params = {"epoch": epoch,"window_size": window_size,"batch_size": batch_size,"learning_rate": learning_rate}
            result_base_dir = os.path.join(BASE_DIR,result_path,task_name_test)
            pattern = f"train_window_{st.session_state.window_size}_*"
            result_window_dirs = [
                d for d in glob(os.path.join(result_base_dir , pattern))
                if os.path.isdir(d)]
            latest_result_dir = max(result_window_dirs, key=os.path.getctime)
            dest_path = os.path.join(latest_result_dir,"data_preprocessing_parameters.json")
            shutil.move(save_path, dest_path)
            
            save_path = os.path.join(BASE_DIR, result_path, task_name_test, "model_training_parameters.json")
            with open(save_path, "w", encoding="utf-8") as f:
                json.dump(train_params, f, indent=4)
            result_base_dir = os.path.join(BASE_DIR,result_path,task_name_test)
            pattern = f"train_window_{st.session_state.window_size}_*"
            result_window_dirs = [
                d for d in glob(os.path.join(result_base_dir , pattern))
                if os.path.isdir(d)]
            latest_result_dir = max(result_window_dirs, key=os.path.getctime)
            dest_path = os.path.join(latest_result_dir,"model_training_parameters.json")
            shutil.move(save_path, dest_path)

        except FileNotFoundError:
            st.warning("ğŸ“ ê²°ê³¼ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¬í•™ìŠµì„ ì‹¤í–‰í•˜ì„¸ìš”.")

        except ValueError:
            st.warning("âš ï¸ í•™ìŠµ ê²°ê³¼ ë””ë ‰í† ë¦¬ê°€ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì¬í•™ìŠµì„ ì‹¤í–‰í•˜ì„¸ìš”.")

        except Exception as e:
            st.error(f"âŒ íŒŒë¼ë¯¸í„° ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                        
#######################################################################################################################################################


